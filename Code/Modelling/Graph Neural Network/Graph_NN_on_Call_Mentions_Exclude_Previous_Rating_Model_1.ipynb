{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Graph NN on Call Mentions\n",
    "\n",
    "Considering direct mentions of companies in calls, construct a network of calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from Inductive_Graph_NN_Functions import *\n",
    "model_name = 'exclude_previous_rating_model_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Feature and Class Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>fixed_quarter_date</th>\n",
       "      <th>earnings_call_date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>rating_date</th>\n",
       "      <th>Next Rating</th>\n",
       "      <th>Next Rating Date</th>\n",
       "      <th>Previous Rating</th>\n",
       "      <th>Previous Rating Date</th>\n",
       "      <th>next_rating_date_or_end_of_data</th>\n",
       "      <th>...</th>\n",
       "      <th>Undrst</th>\n",
       "      <th>PN</th>\n",
       "      <th>SW</th>\n",
       "      <th>AP</th>\n",
       "      <th>OU</th>\n",
       "      <th>TONE1</th>\n",
       "      <th>num_q_by_len</th>\n",
       "      <th>pos_score_finbert</th>\n",
       "      <th>train_test_80_20</th>\n",
       "      <th>node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>2014-07-22</td>\n",
       "      <td>AA</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>...</td>\n",
       "      <td>131.0</td>\n",
       "      <td>5.518519</td>\n",
       "      <td>15.261905</td>\n",
       "      <td>2.661290</td>\n",
       "      <td>2.778626</td>\n",
       "      <td>3.188264</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.765917</td>\n",
       "      <td>test</td>\n",
       "      <td>AAPL : 2014-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>2014-10-20</td>\n",
       "      <td>AA</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>...</td>\n",
       "      <td>152.0</td>\n",
       "      <td>5.348485</td>\n",
       "      <td>15.934783</td>\n",
       "      <td>3.296482</td>\n",
       "      <td>3.059211</td>\n",
       "      <td>3.681858</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.731819</td>\n",
       "      <td>test</td>\n",
       "      <td>AAPL : 2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>2015-01-27</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>AA</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>...</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.927711</td>\n",
       "      <td>8.113636</td>\n",
       "      <td>2.841346</td>\n",
       "      <td>3.099338</td>\n",
       "      <td>1.307366</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.690750</td>\n",
       "      <td>train</td>\n",
       "      <td>AAPL : 2015-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>2015-04-27</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-06-02</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>...</td>\n",
       "      <td>135.0</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>9.142857</td>\n",
       "      <td>2.640187</td>\n",
       "      <td>3.074074</td>\n",
       "      <td>2.025933</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.822168</td>\n",
       "      <td>train</td>\n",
       "      <td>AAPL : 2015-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>2015-07-21</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>AA</td>\n",
       "      <td>2016-05-20</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-06-02</td>\n",
       "      <td>2016-05-20</td>\n",
       "      <td>...</td>\n",
       "      <td>148.0</td>\n",
       "      <td>4.209877</td>\n",
       "      <td>10.442857</td>\n",
       "      <td>2.579909</td>\n",
       "      <td>3.033784</td>\n",
       "      <td>1.815531</td>\n",
       "      <td>0.003915</td>\n",
       "      <td>0.808114</td>\n",
       "      <td>test</td>\n",
       "      <td>AAPL : 2015-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5484</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>...</td>\n",
       "      <td>148.0</td>\n",
       "      <td>3.611650</td>\n",
       "      <td>15.634615</td>\n",
       "      <td>2.911215</td>\n",
       "      <td>2.013514</td>\n",
       "      <td>1.744657</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.895791</td>\n",
       "      <td>train</td>\n",
       "      <td>ZTS : 2015-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>...</td>\n",
       "      <td>222.0</td>\n",
       "      <td>3.766917</td>\n",
       "      <td>15.848101</td>\n",
       "      <td>2.791667</td>\n",
       "      <td>1.779279</td>\n",
       "      <td>1.596294</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>0.929419</td>\n",
       "      <td>train</td>\n",
       "      <td>ZTS : 2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5486</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-02-16</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>...</td>\n",
       "      <td>217.0</td>\n",
       "      <td>3.565517</td>\n",
       "      <td>17.506849</td>\n",
       "      <td>2.926829</td>\n",
       "      <td>2.161290</td>\n",
       "      <td>2.287146</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>0.585873</td>\n",
       "      <td>test</td>\n",
       "      <td>ZTS : 2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5487</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>2016-05-04</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>...</td>\n",
       "      <td>215.0</td>\n",
       "      <td>3.572650</td>\n",
       "      <td>15.235294</td>\n",
       "      <td>3.023715</td>\n",
       "      <td>2.088372</td>\n",
       "      <td>1.739992</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.666177</td>\n",
       "      <td>train</td>\n",
       "      <td>ZTS : 2016-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5488</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2016-08-03</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>...</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2.858896</td>\n",
       "      <td>12.395349</td>\n",
       "      <td>2.840000</td>\n",
       "      <td>2.288557</td>\n",
       "      <td>0.976340</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.748188</td>\n",
       "      <td>train</td>\n",
       "      <td>ZTS : 2016-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5489 rows × 173 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker fixed_quarter_date earnings_call_date Rating rating_date  \\\n",
       "0      AAPL         2014-10-01         2014-07-22     AA  2014-05-27   \n",
       "1      AAPL         2015-01-01         2014-10-20     AA  2014-05-27   \n",
       "2      AAPL         2015-04-01         2015-01-27     AA  2015-02-18   \n",
       "3      AAPL         2015-07-01         2015-04-27     AA  2015-06-02   \n",
       "4      AAPL         2015-10-01         2015-07-21     AA  2015-08-25   \n",
       "...     ...                ...                ...    ...         ...   \n",
       "5484    ZTS         2015-10-01         2015-08-04    BBB  2015-01-30   \n",
       "5485    ZTS         2016-01-01         2015-11-03    BBB  2015-11-03   \n",
       "5486    ZTS         2016-04-01         2016-02-16    BBB  2016-01-22   \n",
       "5487    ZTS         2016-07-01         2016-05-04    BBB  2016-01-22   \n",
       "5488    ZTS         2016-10-01         2016-08-03    BBB  2016-01-22   \n",
       "\n",
       "     Next Rating Next Rating Date Previous Rating Previous Rating Date  \\\n",
       "0             AA       2015-02-18             AAA           2014-04-24   \n",
       "1             AA       2015-02-18             AAA           2014-04-24   \n",
       "2             AA       2015-05-28              AA           2014-05-27   \n",
       "3             AA       2015-08-25              AA           2015-05-28   \n",
       "4             AA       2016-05-20              AA           2015-06-02   \n",
       "...          ...              ...             ...                  ...   \n",
       "5484         BBB       2015-11-03             BBB           2014-01-31   \n",
       "5485         BBB       2016-01-22             BBB           2015-01-30   \n",
       "5486         BBB       2016-12-23             BBB           2015-11-03   \n",
       "5487         BBB       2016-12-23             BBB           2015-11-03   \n",
       "5488         BBB       2016-12-23             BBB           2015-11-03   \n",
       "\n",
       "     next_rating_date_or_end_of_data  ...  Undrst        PN         SW  \\\n",
       "0                         2015-02-18  ...   131.0  5.518519  15.261905   \n",
       "1                         2015-02-18  ...   152.0  5.348485  15.934783   \n",
       "2                         2015-05-28  ...   151.0  3.927711   8.113636   \n",
       "3                         2015-08-25  ...   135.0  5.250000   9.142857   \n",
       "4                         2016-05-20  ...   148.0  4.209877  10.442857   \n",
       "...                              ...  ...     ...       ...        ...   \n",
       "5484                      2015-11-03  ...   148.0  3.611650  15.634615   \n",
       "5485                      2016-01-22  ...   222.0  3.766917  15.848101   \n",
       "5486                      2016-12-23  ...   217.0  3.565517  17.506849   \n",
       "5487                      2016-12-23  ...   215.0  3.572650  15.235294   \n",
       "5488                      2016-12-23  ...   201.0  2.858896  12.395349   \n",
       "\n",
       "            AP        OU     TONE1 num_q_by_len pos_score_finbert  \\\n",
       "0     2.661290  2.778626  3.188264     0.003822          0.765917   \n",
       "1     3.296482  3.059211  3.681858     0.002766          0.731819   \n",
       "2     2.841346  3.099338  1.307366     0.004628          0.690750   \n",
       "3     2.640187  3.074074  2.025933     0.003861          0.822168   \n",
       "4     2.579909  3.033784  1.815531     0.003915          0.808114   \n",
       "...        ...       ...       ...          ...               ...   \n",
       "5484  2.911215  2.013514  1.744657     0.001458          0.895791   \n",
       "5485  2.791667  1.779279  1.596294     0.003859          0.929419   \n",
       "5486  2.926829  2.161290  2.287146     0.003928          0.585873   \n",
       "5487  3.023715  2.088372  1.739992     0.003182          0.666177   \n",
       "5488  2.840000  2.288557  0.976340     0.002697          0.748188   \n",
       "\n",
       "     train_test_80_20               node  \n",
       "0                test  AAPL : 2014-10-01  \n",
       "1                test  AAPL : 2015-01-01  \n",
       "2               train  AAPL : 2015-04-01  \n",
       "3               train  AAPL : 2015-07-01  \n",
       "4                test  AAPL : 2015-10-01  \n",
       "...               ...                ...  \n",
       "5484            train   ZTS : 2015-10-01  \n",
       "5485            train   ZTS : 2016-01-01  \n",
       "5486             test   ZTS : 2016-04-01  \n",
       "5487            train   ZTS : 2016-07-01  \n",
       "5488            train   ZTS : 2016-10-01  \n",
       "\n",
       "[5489 rows x 173 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load feature and class data\n",
    "feature_and_class_df = load_feature_and_class_data()\n",
    "feature_and_class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load column names\n",
    "numeric_feature_columns, cat_feature_columns, target_column, custom_mapping = get_column_names_and_mapping(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names: \n",
      "['num__Altman_Z']\n"
     ]
    }
   ],
   "source": [
    "# Prepare matrices\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, feature_names, train_ticker_by_fixed_quarter_date, test_ticker_by_fixed_quarter_date = prepare_matrices(feature_and_class_df, numeric_feature_columns, cat_feature_columns, target_column, custom_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      num__Altman_Z\n",
      "0          1.685950\n",
      "1          3.174735\n",
      "2          3.007245\n",
      "3          3.150117\n",
      "4          3.036072\n",
      "...             ...\n",
      "4392       1.311536\n",
      "4393       1.295700\n",
      "4394       1.056366\n",
      "4395       0.899041\n",
      "4396       1.056903\n",
      "\n",
      "[4397 rows x 1 columns]\n",
      "finalized dfs\n",
      "      num__Altman_Z  Rating               node\n",
      "0          1.685950       1  AAPL : 2015-04-01\n",
      "1          3.174735       1  AAPL : 2015-07-01\n",
      "2          3.007245       1  AAPL : 2016-01-01\n",
      "3          3.150117       1  AAPL : 2016-04-01\n",
      "4          3.036072       1  AAPL : 2016-07-01\n",
      "...             ...     ...                ...\n",
      "4392       1.311536       3   ZTS : 2015-07-01\n",
      "4393       1.295700       3   ZTS : 2015-10-01\n",
      "4394       1.056366       3   ZTS : 2016-01-01\n",
      "4395       0.899041       3   ZTS : 2016-07-01\n",
      "4396       1.056903       3   ZTS : 2016-10-01\n",
      "\n",
      "[4397 rows x 3 columns]\n",
      "      num__Altman_Z  Rating               node\n",
      "0          1.949561       1  AAPL : 2014-10-01\n",
      "1          3.061766       1  AAPL : 2015-01-01\n",
      "2          3.051469       1  AAPL : 2015-10-01\n",
      "3         -0.092929       2  ABBV : 2015-10-01\n",
      "4         -0.446969       2   ABC : 2013-04-01\n",
      "...             ...     ...                ...\n",
      "1087       0.931329       3   YUM : 2014-04-01\n",
      "1088       0.275978       4   YUM : 2016-07-01\n",
      "1089      -0.483129       5  ZBRA : 2016-04-01\n",
      "1090       0.647424       3   ZTS : 2014-10-01\n",
      "1091       0.726634       3   ZTS : 2016-04-01\n",
      "\n",
      "[1092 rows x 3 columns]\n",
      "missing values of target_column in train_and_val_df or test_df?\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Assemble back into dataframes\n",
    "\n",
    "# Train and val\n",
    "train_and_val_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "print(train_and_val_df)\n",
    "# Add y_train\n",
    "train_and_val_df[target_column] = y_train.reset_index(drop=True)\n",
    "# Add ticker by fixed quarter date\n",
    "train_and_val_df = pd.concat([train_ticker_by_fixed_quarter_date.reset_index(drop=True).sort_values(['ticker', 'fixed_quarter_date']), train_and_val_df], axis=1)\n",
    "# Add node by merging with feature_and_class_df (inner join)\n",
    "train_and_val_df = train_and_val_df.merge(feature_and_class_df[['ticker', 'fixed_quarter_date', 'node']], on=['ticker', 'fixed_quarter_date'], how='inner')\n",
    "# Drop ticker and fixed_quarter_date\n",
    "train_and_val_df = train_and_val_df.drop(['ticker', 'fixed_quarter_date'], axis=1)\n",
    "\n",
    "# Test\n",
    "test_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "# Add y_test\n",
    "test_df[target_column] = y_test.reset_index(drop=True)\n",
    "# Add ticker by fixed quarter date\n",
    "test_df = pd.concat([test_ticker_by_fixed_quarter_date.reset_index(drop=True).sort_values(['ticker', 'fixed_quarter_date']), test_df], axis=1)\n",
    "# Add node by merging with feature_and_class_df (inner join)\n",
    "test_df = test_df.merge(feature_and_class_df[['ticker', 'fixed_quarter_date', 'node']], on=['ticker', 'fixed_quarter_date'], how='inner')\n",
    "# Drop ticker and fixed_quarter_date\n",
    "test_df = test_df.drop(['ticker', 'fixed_quarter_date'], axis=1)\n",
    "\n",
    "print('finalized dfs')\n",
    "print(train_and_val_df)\n",
    "print(test_df)\n",
    "print('missing values of target_column in train_and_val_df or test_df?')\n",
    "print(train_and_val_df[target_column].isnull().sum() > 0)\n",
    "print(test_df[target_column].isnull().sum() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pairwise Mentions Data\n",
    "\n",
    "Note: it's OK if we lose observations here, because on some fixed quarter dates we don't have data for both companies in a mention link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num obs\n",
      "2725\n",
      "num obs\n",
      "2725\n",
      "num obs\n",
      "1782\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEE : 2012-04-01</td>\n",
       "      <td>MCO : 2012-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PNM : 2012-04-01</td>\n",
       "      <td>MCO : 2012-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADP : 2012-04-01</td>\n",
       "      <td>MCO : 2012-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEE : 2012-07-01</td>\n",
       "      <td>LNT : 2012-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NEE : 2014-04-01</td>\n",
       "      <td>MCO : 2014-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>KOS : 2016-01-01</td>\n",
       "      <td>COP : 2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>KOS : 2016-10-01</td>\n",
       "      <td>HES : 2016-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>KSS : 2014-04-01</td>\n",
       "      <td>FDX : 2014-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>KTOS : 2014-10-01</td>\n",
       "      <td>NOC : 2014-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>KTOS : 2015-01-01</td>\n",
       "      <td>NOC : 2015-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1782 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    src               dst\n",
       "0      NEE : 2012-04-01  MCO : 2012-04-01\n",
       "1      PNM : 2012-04-01  MCO : 2012-04-01\n",
       "2      ADP : 2012-04-01  MCO : 2012-04-01\n",
       "3      NEE : 2012-07-01  LNT : 2012-07-01\n",
       "4      NEE : 2014-04-01  MCO : 2014-04-01\n",
       "...                 ...               ...\n",
       "1777   KOS : 2016-01-01  COP : 2016-01-01\n",
       "1778   KOS : 2016-10-01  HES : 2016-10-01\n",
       "1779   KSS : 2014-04-01  FDX : 2014-04-01\n",
       "1780  KTOS : 2014-10-01  NOC : 2014-10-01\n",
       "1781  KTOS : 2015-01-01  NOC : 2015-01-01\n",
       "\n",
       "[1782 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dst_df = load_src_dst_data()\n",
    "print('num obs')\n",
    "print(len(src_dst_df))\n",
    "# Convert fixed_quarter_date to a string\n",
    "src_dst_df['fixed_quarter_date'] = src_dst_df['fixed_quarter_date'].astype(str)\n",
    "feature_and_class_df['fixed_quarter_date'] = feature_and_class_df['fixed_quarter_date'].astype(str)\n",
    "# Join with feature_and_class_df to get node for src_ticker and dst_ticker\n",
    "src_dst_df = src_dst_df.merge(feature_and_class_df[['ticker', 'fixed_quarter_date', 'node']], left_on=['src_ticker', 'fixed_quarter_date'], right_on=['ticker', 'fixed_quarter_date'], how='inner').rename(columns={'node': 'src_node'})\n",
    "print('num obs')\n",
    "print(len(src_dst_df))\n",
    "src_dst_df = src_dst_df.merge(feature_and_class_df[['ticker', 'fixed_quarter_date', 'node']], left_on=['dst_ticker', 'fixed_quarter_date'], right_on=['ticker', 'fixed_quarter_date'], how='inner').rename(columns={'node': 'dst_node'})\n",
    "print('num obs')\n",
    "print(len(src_dst_df))\n",
    "# Limit columns to just src_node and dst_node, rename to src and dst\n",
    "src_dst_df = src_dst_df[['src_node', 'dst_node']].rename(columns={'src_node': 'src', 'dst_node': 'dst'})\n",
    "src_dst_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edits to train and val and test dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keeping only items that are connected/have a node in src or dst in src_dst_df\n",
      "drop items in classes with only one node\n",
      "length of train_and_val_df\n",
      "1574\n",
      "new length of train_and_val_df\n",
      "1573\n",
      "keeping only src and dst that are in train_and_val_df or test_df\n",
      "length of src_dst_df\n",
      "1782\n",
      "new length of src_dst_df\n",
      "1781\n",
      "keeping only items that are connected/have a node in src or dst in src_dst_df again\n",
      "length of train_and_val_df\n",
      "1573\n",
      "length of test_df\n",
      "376\n",
      "new length of train_and_val_df\n",
      "1573\n",
      "new length of test_df\n",
      "376\n"
     ]
    }
   ],
   "source": [
    "# Limit train_and_val_df and test_df to just items with a node in one of the src or dst columns\n",
    "print('keeping only items that are connected/have a node in src or dst in src_dst_df')\n",
    "train_and_val_df = train_and_val_df[train_and_val_df['node'].isin(src_dst_df['src']) | train_and_val_df['node'].isin(src_dst_df['dst'])]\n",
    "test_df = test_df[test_df['node'].isin(src_dst_df['src']) | test_df['node'].isin(src_dst_df['dst'])]\n",
    "\n",
    "# Drop any items that belong to target_column values with only one node\n",
    "print('drop items in classes with only one node')\n",
    "print('length of train_and_val_df')\n",
    "print(len(train_and_val_df))\n",
    "train_and_val_df = train_and_val_df.groupby(target_column).filter(lambda x: len(x) > 1).reset_index(drop=True)\n",
    "print('new length of train_and_val_df')\n",
    "print(len(train_and_val_df))\n",
    "\n",
    "# Limit src and dst to just nodes in train_and_val_df or test_df\n",
    "print('keeping only src and dst that are in train_and_val_df or test_df')\n",
    "print('length of src_dst_df')\n",
    "print(len(src_dst_df))\n",
    "src_dst_df = src_dst_df[src_dst_df['src'].isin(train_and_val_df['node']) | src_dst_df['src'].isin(test_df['node'])]\n",
    "src_dst_df = src_dst_df[src_dst_df['dst'].isin(train_and_val_df['node']) | src_dst_df['dst'].isin(test_df['node'])]\n",
    "print('new length of src_dst_df')\n",
    "print(len(src_dst_df))\n",
    "\n",
    "# Limit train_and_val_df and test_df to just items with a node in one of the src or dst columns\n",
    "print('keeping only items that are connected/have a node in src or dst in src_dst_df again')\n",
    "print('length of train_and_val_df')\n",
    "print(len(train_and_val_df))\n",
    "print('length of test_df')\n",
    "print(len(test_df))\n",
    "train_and_val_df = train_and_val_df[train_and_val_df['node'].isin(src_dst_df['src']) | train_and_val_df['node'].isin(src_dst_df['dst'])]\n",
    "test_df = test_df[test_df['node'].isin(src_dst_df['src']) | test_df['node'].isin(src_dst_df['dst'])]\n",
    "print('new length of train_and_val_df')\n",
    "print(len(train_and_val_df))\n",
    "print('new length of test_df')\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Node as Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all values of node in train_and_val_df and test_df\n",
    "all_nodes = list(set(train_and_val_df['node']) | set(test_df['node']))\n",
    "# Encode as integers, create a mapping\n",
    "node_to_int = {node: i for i, node in enumerate(all_nodes)}\n",
    "# Add to train_and_val_df and test_df as a replacement of node\n",
    "train_and_val_df['node'] = train_and_val_df['node'].map(node_to_int)\n",
    "test_df['node'] = test_df['node'].map(node_to_int)\n",
    "# Same for src and dst\n",
    "src_dst_df['src'] = src_dst_df['src'].map(node_to_int)\n",
    "src_dst_df['dst'] = src_dst_df['dst'].map(node_to_int)\n",
    "# Convert dictionary of node_to_int to df\n",
    "node_to_int_df = pd.DataFrame(list(node_to_int.items()), columns=['node', 'node_int'])\n",
    "# Save to disk\n",
    "node_to_int_df.to_excel('../../../Output/Modelling/Graph Neural Network/' + model_name + '/' + 'node_to_int.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Inductive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Further slice the train dataset into train and validation datasets.\n",
      "The training data has shape: (1258, 3).\n",
      "The validation data has shape: (315, 3).\n",
      "The test data has shape: (376, 3).\n",
      "Generate train, validation, and test masks.\n",
      "sum of train mask\n",
      "tensor(1258)\n",
      "sum of val mask\n",
      "tensor(315)\n",
      "sum of test mask\n",
      "tensor(376)\n",
      "Number of nodes = 1949\n",
      "Number of features for each node = 1\n",
      "Number of classes = 9.\n",
      "Initializing Model\n",
      "Initialized Model\n",
      "NodeClassification(\n",
      "  (gconv_model): GraphSAGEModel(\n",
      "    (layers): ModuleList(\n",
      "      (0): SAGEConv(\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (fc_pool): Linear(in_features=1, out_features=1, bias=True)\n",
      "        (fc_neigh): Linear(in_features=1, out_features=64, bias=False)\n",
      "        (fc_self): Linear(in_features=1, out_features=64, bias=True)\n",
      "      )\n",
      "      (1): SAGEConv(\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (fc_pool): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_neigh): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (fc_self): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (2): SAGEConv(\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (fc_pool): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_neigh): Linear(in_features=64, out_features=9, bias=False)\n",
      "        (fc_self): Linear(in_features=64, out_features=9, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loss_fcn): CrossEntropyLoss()\n",
      ")\n",
      "<generator object Module.parameters at 0x0000015E836EC120>\n",
      "GPU available:  False\n",
      "Starting Model training and prediction\n",
      "Epoch 00000 | Time(s) 0.0740 | Training Accuracy 0.2440 | Training Loss 2.1039 | Training F1 0.1514 | Validation Accuracy 0.2254 | Validation F1 0.1389\n",
      "Epoch 00001 | Time(s) 0.0698 | Training Accuracy 0.2409 | Training Loss 2.1078 | Training F1 0.1506 | Validation Accuracy 0.2317 | Validation F1 0.1407\n",
      "Epoch 00002 | Time(s) 0.0686 | Training Accuracy 0.2727 | Training Loss 1.8480 | Training F1 0.2236 | Validation Accuracy 0.2603 | Validation F1 0.2192\n",
      "Epoch 00003 | Time(s) 0.0702 | Training Accuracy 0.3267 | Training Loss 1.6893 | Training F1 0.1812 | Validation Accuracy 0.3238 | Validation F1 0.1815\n",
      "Epoch 00004 | Time(s) 0.0697 | Training Accuracy 0.3275 | Training Loss 1.6628 | Training F1 0.1727 | Validation Accuracy 0.3302 | Validation F1 0.1805\n",
      "Epoch 00005 | Time(s) 0.0674 | Training Accuracy 0.3355 | Training Loss 1.6671 | Training F1 0.1907 | Validation Accuracy 0.3333 | Validation F1 0.1912\n",
      "Epoch 00006 | Time(s) 0.0701 | Training Accuracy 0.3458 | Training Loss 1.6153 | Training F1 0.2328 | Validation Accuracy 0.3365 | Validation F1 0.2223\n",
      "Epoch 00007 | Time(s) 0.0703 | Training Accuracy 0.3466 | Training Loss 1.5688 | Training F1 0.3097 | Validation Accuracy 0.3111 | Validation F1 0.2738\n",
      "Epoch 00008 | Time(s) 0.0712 | Training Accuracy 0.3323 | Training Loss 1.5630 | Training F1 0.2973 | Validation Accuracy 0.3206 | Validation F1 0.2807\n",
      "Epoch 00009 | Time(s) 0.0694 | Training Accuracy 0.3315 | Training Loss 1.5678 | Training F1 0.2956 | Validation Accuracy 0.3206 | Validation F1 0.2802\n",
      "Epoch 00010 | Time(s) 0.0695 | Training Accuracy 0.3466 | Training Loss 1.5564 | Training F1 0.3155 | Validation Accuracy 0.3460 | Validation F1 0.3084\n",
      "Epoch 00011 | Time(s) 0.0696 | Training Accuracy 0.3601 | Training Loss 1.5365 | Training F1 0.3166 | Validation Accuracy 0.3333 | Validation F1 0.2886\n",
      "Epoch 00012 | Time(s) 0.0699 | Training Accuracy 0.3641 | Training Loss 1.5233 | Training F1 0.3070 | Validation Accuracy 0.3492 | Validation F1 0.2896\n",
      "Epoch 00013 | Time(s) 0.0692 | Training Accuracy 0.3617 | Training Loss 1.5232 | Training F1 0.2997 | Validation Accuracy 0.3460 | Validation F1 0.2826\n",
      "Epoch 00014 | Time(s) 0.0687 | Training Accuracy 0.3625 | Training Loss 1.5274 | Training F1 0.2986 | Validation Accuracy 0.3492 | Validation F1 0.2856\n",
      "Epoch 00015 | Time(s) 0.0688 | Training Accuracy 0.3601 | Training Loss 1.5220 | Training F1 0.3021 | Validation Accuracy 0.3460 | Validation F1 0.2913\n",
      "Epoch 00016 | Time(s) 0.0694 | Training Accuracy 0.3760 | Training Loss 1.5111 | Training F1 0.3322 | Validation Accuracy 0.3397 | Validation F1 0.2963\n",
      "Epoch 00017 | Time(s) 0.0703 | Training Accuracy 0.3625 | Training Loss 1.5063 | Training F1 0.3305 | Validation Accuracy 0.3111 | Validation F1 0.2777\n",
      "Epoch 00018 | Time(s) 0.0699 | Training Accuracy 0.3657 | Training Loss 1.5061 | Training F1 0.3312 | Validation Accuracy 0.3206 | Validation F1 0.2871\n",
      "Epoch 00019 | Time(s) 0.0701 | Training Accuracy 0.3720 | Training Loss 1.5037 | Training F1 0.3318 | Validation Accuracy 0.3365 | Validation F1 0.2937\n",
      "Epoch 00020 | Time(s) 0.0696 | Training Accuracy 0.3824 | Training Loss 1.4984 | Training F1 0.3307 | Validation Accuracy 0.3175 | Validation F1 0.2708\n",
      "Epoch 00021 | Time(s) 0.0697 | Training Accuracy 0.3816 | Training Loss 1.4936 | Training F1 0.3238 | Validation Accuracy 0.3143 | Validation F1 0.2619\n",
      "Epoch 00022 | Time(s) 0.0701 | Training Accuracy 0.3800 | Training Loss 1.4911 | Training F1 0.3192 | Validation Accuracy 0.3175 | Validation F1 0.2654\n",
      "Epoch 00023 | Time(s) 0.0696 | Training Accuracy 0.3824 | Training Loss 1.4896 | Training F1 0.3250 | Validation Accuracy 0.3079 | Validation F1 0.2598\n",
      "Epoch 00024 | Time(s) 0.0702 | Training Accuracy 0.3839 | Training Loss 1.4871 | Training F1 0.3314 | Validation Accuracy 0.3238 | Validation F1 0.2756\n",
      "Epoch 00025 | Time(s) 0.0694 | Training Accuracy 0.3784 | Training Loss 1.4831 | Training F1 0.3320 | Validation Accuracy 0.3302 | Validation F1 0.2828\n",
      "Epoch 00026 | Time(s) 0.0694 | Training Accuracy 0.3792 | Training Loss 1.4796 | Training F1 0.3382 | Validation Accuracy 0.3302 | Validation F1 0.2850\n",
      "Epoch 00027 | Time(s) 0.0696 | Training Accuracy 0.3768 | Training Loss 1.4778 | Training F1 0.3375 | Validation Accuracy 0.3302 | Validation F1 0.2840\n",
      "Epoch 00028 | Time(s) 0.0692 | Training Accuracy 0.3863 | Training Loss 1.4755 | Training F1 0.3445 | Validation Accuracy 0.3238 | Validation F1 0.2788\n",
      "Epoch 00029 | Time(s) 0.0690 | Training Accuracy 0.3863 | Training Loss 1.4724 | Training F1 0.3399 | Validation Accuracy 0.3206 | Validation F1 0.2747\n",
      "Epoch 00030 | Time(s) 0.0689 | Training Accuracy 0.3839 | Training Loss 1.4694 | Training F1 0.3352 | Validation Accuracy 0.3238 | Validation F1 0.2768\n",
      "Epoch 00031 | Time(s) 0.0684 | Training Accuracy 0.3855 | Training Loss 1.4671 | Training F1 0.3414 | Validation Accuracy 0.3206 | Validation F1 0.2728\n",
      "Epoch 00032 | Time(s) 0.0686 | Training Accuracy 0.3935 | Training Loss 1.4646 | Training F1 0.3536 | Validation Accuracy 0.3175 | Validation F1 0.2704\n",
      "Epoch 00033 | Time(s) 0.0686 | Training Accuracy 0.4006 | Training Loss 1.4621 | Training F1 0.3627 | Validation Accuracy 0.3175 | Validation F1 0.2712\n",
      "Epoch 00034 | Time(s) 0.0686 | Training Accuracy 0.3975 | Training Loss 1.4602 | Training F1 0.3596 | Validation Accuracy 0.3143 | Validation F1 0.2682\n",
      "Epoch 00035 | Time(s) 0.0684 | Training Accuracy 0.3943 | Training Loss 1.4576 | Training F1 0.3539 | Validation Accuracy 0.3175 | Validation F1 0.2700\n",
      "Epoch 00036 | Time(s) 0.0685 | Training Accuracy 0.3927 | Training Loss 1.4552 | Training F1 0.3495 | Validation Accuracy 0.3238 | Validation F1 0.2757\n",
      "Epoch 00037 | Time(s) 0.0685 | Training Accuracy 0.3990 | Training Loss 1.4530 | Training F1 0.3567 | Validation Accuracy 0.3206 | Validation F1 0.2713\n",
      "Epoch 00038 | Time(s) 0.0685 | Training Accuracy 0.4030 | Training Loss 1.4507 | Training F1 0.3624 | Validation Accuracy 0.3206 | Validation F1 0.2736\n",
      "Epoch 00039 | Time(s) 0.0681 | Training Accuracy 0.4014 | Training Loss 1.4483 | Training F1 0.3636 | Validation Accuracy 0.3175 | Validation F1 0.2713\n",
      "Epoch 00040 | Time(s) 0.0681 | Training Accuracy 0.4030 | Training Loss 1.4461 | Training F1 0.3647 | Validation Accuracy 0.3238 | Validation F1 0.2776\n",
      "Epoch 00041 | Time(s) 0.0680 | Training Accuracy 0.3998 | Training Loss 1.4438 | Training F1 0.3622 | Validation Accuracy 0.3175 | Validation F1 0.2699\n",
      "Epoch 00042 | Time(s) 0.0681 | Training Accuracy 0.3959 | Training Loss 1.4412 | Training F1 0.3568 | Validation Accuracy 0.3175 | Validation F1 0.2697\n",
      "Epoch 00043 | Time(s) 0.0681 | Training Accuracy 0.3967 | Training Loss 1.4386 | Training F1 0.3574 | Validation Accuracy 0.3206 | Validation F1 0.2741\n",
      "Epoch 00044 | Time(s) 0.0682 | Training Accuracy 0.3959 | Training Loss 1.4362 | Training F1 0.3572 | Validation Accuracy 0.3143 | Validation F1 0.2702\n",
      "Epoch 00045 | Time(s) 0.0683 | Training Accuracy 0.4014 | Training Loss 1.4336 | Training F1 0.3646 | Validation Accuracy 0.3206 | Validation F1 0.2764\n",
      "Epoch 00046 | Time(s) 0.0681 | Training Accuracy 0.4014 | Training Loss 1.4311 | Training F1 0.3658 | Validation Accuracy 0.3175 | Validation F1 0.2750\n",
      "Epoch 00047 | Time(s) 0.0677 | Training Accuracy 0.4006 | Training Loss 1.4288 | Training F1 0.3631 | Validation Accuracy 0.3206 | Validation F1 0.2743\n",
      "Epoch 00048 | Time(s) 0.0681 | Training Accuracy 0.3990 | Training Loss 1.4261 | Training F1 0.3606 | Validation Accuracy 0.3270 | Validation F1 0.2814\n",
      "Epoch 00049 | Time(s) 0.0680 | Training Accuracy 0.4006 | Training Loss 1.4235 | Training F1 0.3642 | Validation Accuracy 0.3238 | Validation F1 0.2793\n",
      "Epoch 00050 | Time(s) 0.0679 | Training Accuracy 0.4062 | Training Loss 1.4208 | Training F1 0.3703 | Validation Accuracy 0.3175 | Validation F1 0.2744\n",
      "Epoch 00051 | Time(s) 0.0680 | Training Accuracy 0.4062 | Training Loss 1.4184 | Training F1 0.3697 | Validation Accuracy 0.3175 | Validation F1 0.2716\n",
      "Epoch 00052 | Time(s) 0.0682 | Training Accuracy 0.4030 | Training Loss 1.4158 | Training F1 0.3670 | Validation Accuracy 0.3206 | Validation F1 0.2760\n",
      "Epoch 00053 | Time(s) 0.0683 | Training Accuracy 0.4030 | Training Loss 1.4132 | Training F1 0.3672 | Validation Accuracy 0.3175 | Validation F1 0.2739\n",
      "Epoch 00054 | Time(s) 0.0686 | Training Accuracy 0.4062 | Training Loss 1.4104 | Training F1 0.3678 | Validation Accuracy 0.3175 | Validation F1 0.2718\n",
      "Epoch 00055 | Time(s) 0.0685 | Training Accuracy 0.4062 | Training Loss 1.4078 | Training F1 0.3712 | Validation Accuracy 0.3175 | Validation F1 0.2757\n",
      "Epoch 00056 | Time(s) 0.0686 | Training Accuracy 0.4078 | Training Loss 1.4048 | Training F1 0.3732 | Validation Accuracy 0.3238 | Validation F1 0.2815\n",
      "Epoch 00057 | Time(s) 0.0685 | Training Accuracy 0.4086 | Training Loss 1.4021 | Training F1 0.3752 | Validation Accuracy 0.3206 | Validation F1 0.2771\n",
      "Epoch 00058 | Time(s) 0.0684 | Training Accuracy 0.4141 | Training Loss 1.3990 | Training F1 0.3801 | Validation Accuracy 0.3206 | Validation F1 0.2760\n",
      "Epoch 00059 | Time(s) 0.0684 | Training Accuracy 0.4070 | Training Loss 1.3959 | Training F1 0.3741 | Validation Accuracy 0.3175 | Validation F1 0.2731\n",
      "Epoch 00060 | Time(s) 0.0684 | Training Accuracy 0.4173 | Training Loss 1.3929 | Training F1 0.3855 | Validation Accuracy 0.3143 | Validation F1 0.2731\n",
      "Epoch 00061 | Time(s) 0.0686 | Training Accuracy 0.4118 | Training Loss 1.3899 | Training F1 0.3814 | Validation Accuracy 0.3143 | Validation F1 0.2731\n",
      "Epoch 00062 | Time(s) 0.0686 | Training Accuracy 0.4141 | Training Loss 1.3867 | Training F1 0.3820 | Validation Accuracy 0.3238 | Validation F1 0.2827\n",
      "Epoch 00063 | Time(s) 0.0687 | Training Accuracy 0.4134 | Training Loss 1.3841 | Training F1 0.3863 | Validation Accuracy 0.3238 | Validation F1 0.2825\n",
      "Epoch 00064 | Time(s) 0.0687 | Training Accuracy 0.4189 | Training Loss 1.3817 | Training F1 0.3884 | Validation Accuracy 0.3333 | Validation F1 0.2923\n",
      "Epoch 00065 | Time(s) 0.0687 | Training Accuracy 0.4141 | Training Loss 1.3830 | Training F1 0.3910 | Validation Accuracy 0.3302 | Validation F1 0.2926\n",
      "Epoch 00066 | Time(s) 0.0688 | Training Accuracy 0.4110 | Training Loss 1.3864 | Training F1 0.3766 | Validation Accuracy 0.3270 | Validation F1 0.2756\n",
      "Epoch 00067 | Time(s) 0.0688 | Training Accuracy 0.4173 | Training Loss 1.3840 | Training F1 0.3897 | Validation Accuracy 0.3270 | Validation F1 0.2885\n",
      "Epoch 00068 | Time(s) 0.0690 | Training Accuracy 0.4134 | Training Loss 1.3734 | Training F1 0.3859 | Validation Accuracy 0.3302 | Validation F1 0.2911\n",
      "Epoch 00069 | Time(s) 0.0692 | Training Accuracy 0.4221 | Training Loss 1.3826 | Training F1 0.3909 | Validation Accuracy 0.3206 | Validation F1 0.2771\n",
      "Epoch 00070 | Time(s) 0.0696 | Training Accuracy 0.4197 | Training Loss 1.3713 | Training F1 0.3921 | Validation Accuracy 0.3175 | Validation F1 0.2795\n",
      "Epoch 00071 | Time(s) 0.0699 | Training Accuracy 0.4118 | Training Loss 1.3737 | Training F1 0.3865 | Validation Accuracy 0.3302 | Validation F1 0.2942\n",
      "Epoch 00072 | Time(s) 0.0698 | Training Accuracy 0.4229 | Training Loss 1.3682 | Training F1 0.3966 | Validation Accuracy 0.3238 | Validation F1 0.2852\n",
      "Epoch 00073 | Time(s) 0.0698 | Training Accuracy 0.4229 | Training Loss 1.3644 | Training F1 0.3953 | Validation Accuracy 0.3206 | Validation F1 0.2802\n",
      "Epoch 00074 | Time(s) 0.0698 | Training Accuracy 0.4332 | Training Loss 1.3657 | Training F1 0.4066 | Validation Accuracy 0.3238 | Validation F1 0.2835\n",
      "Epoch 00075 | Time(s) 0.0698 | Training Accuracy 0.4149 | Training Loss 1.3589 | Training F1 0.3890 | Validation Accuracy 0.3302 | Validation F1 0.2926\n",
      "Epoch 00076 | Time(s) 0.0699 | Training Accuracy 0.4300 | Training Loss 1.3613 | Training F1 0.4055 | Validation Accuracy 0.3365 | Validation F1 0.2971\n",
      "Epoch 00077 | Time(s) 0.0699 | Training Accuracy 0.4253 | Training Loss 1.3543 | Training F1 0.3977 | Validation Accuracy 0.3365 | Validation F1 0.2967\n",
      "Epoch 00078 | Time(s) 0.0698 | Training Accuracy 0.4324 | Training Loss 1.3558 | Training F1 0.4074 | Validation Accuracy 0.3206 | Validation F1 0.2795\n",
      "Epoch 00079 | Time(s) 0.0700 | Training Accuracy 0.4213 | Training Loss 1.3512 | Training F1 0.4000 | Validation Accuracy 0.3238 | Validation F1 0.2863\n",
      "Epoch 00080 | Time(s) 0.0704 | Training Accuracy 0.4348 | Training Loss 1.3521 | Training F1 0.4094 | Validation Accuracy 0.3206 | Validation F1 0.2751\n",
      "Epoch 00081 | Time(s) 0.0704 | Training Accuracy 0.4348 | Training Loss 1.3486 | Training F1 0.4089 | Validation Accuracy 0.3206 | Validation F1 0.2811\n",
      "Epoch 00082 | Time(s) 0.0704 | Training Accuracy 0.4285 | Training Loss 1.3470 | Training F1 0.4060 | Validation Accuracy 0.3270 | Validation F1 0.2911\n",
      "Epoch 00083 | Time(s) 0.0706 | Training Accuracy 0.4293 | Training Loss 1.3457 | Training F1 0.4039 | Validation Accuracy 0.3270 | Validation F1 0.2822\n",
      "Epoch 00084 | Time(s) 0.0706 | Training Accuracy 0.4388 | Training Loss 1.3425 | Training F1 0.4127 | Validation Accuracy 0.3238 | Validation F1 0.2848\n",
      "Epoch 00085 | Time(s) 0.0706 | Training Accuracy 0.4396 | Training Loss 1.3411 | Training F1 0.4182 | Validation Accuracy 0.3270 | Validation F1 0.2923\n",
      "Epoch 00086 | Time(s) 0.0705 | Training Accuracy 0.4356 | Training Loss 1.3387 | Training F1 0.4111 | Validation Accuracy 0.3302 | Validation F1 0.2866\n",
      "Epoch 00087 | Time(s) 0.0706 | Training Accuracy 0.4372 | Training Loss 1.3382 | Training F1 0.4131 | Validation Accuracy 0.3270 | Validation F1 0.2856\n",
      "Epoch 00088 | Time(s) 0.0705 | Training Accuracy 0.4372 | Training Loss 1.3344 | Training F1 0.4157 | Validation Accuracy 0.3206 | Validation F1 0.2828\n",
      "Epoch 00089 | Time(s) 0.0709 | Training Accuracy 0.4364 | Training Loss 1.3348 | Training F1 0.4132 | Validation Accuracy 0.3270 | Validation F1 0.2850\n",
      "Epoch 00090 | Time(s) 0.0710 | Training Accuracy 0.4340 | Training Loss 1.3317 | Training F1 0.4105 | Validation Accuracy 0.3175 | Validation F1 0.2753\n",
      "Epoch 00091 | Time(s) 0.0710 | Training Accuracy 0.4372 | Training Loss 1.3304 | Training F1 0.4141 | Validation Accuracy 0.3302 | Validation F1 0.2903\n",
      "Epoch 00092 | Time(s) 0.0709 | Training Accuracy 0.4372 | Training Loss 1.3303 | Training F1 0.4171 | Validation Accuracy 0.3302 | Validation F1 0.2904\n",
      "Epoch 00093 | Time(s) 0.0709 | Training Accuracy 0.4340 | Training Loss 1.3259 | Training F1 0.4101 | Validation Accuracy 0.3333 | Validation F1 0.2912\n",
      "Epoch 00094 | Time(s) 0.0709 | Training Accuracy 0.4420 | Training Loss 1.3260 | Training F1 0.4200 | Validation Accuracy 0.3206 | Validation F1 0.2827\n",
      "Epoch 00095 | Time(s) 0.0709 | Training Accuracy 0.4364 | Training Loss 1.3233 | Training F1 0.4145 | Validation Accuracy 0.3238 | Validation F1 0.2858\n",
      "Epoch 00096 | Time(s) 0.0709 | Training Accuracy 0.4324 | Training Loss 1.3212 | Training F1 0.4105 | Validation Accuracy 0.3333 | Validation F1 0.2914\n",
      "Epoch 00097 | Time(s) 0.0710 | Training Accuracy 0.4412 | Training Loss 1.3207 | Training F1 0.4213 | Validation Accuracy 0.3302 | Validation F1 0.2943\n",
      "Epoch 00098 | Time(s) 0.0710 | Training Accuracy 0.4412 | Training Loss 1.3184 | Training F1 0.4147 | Validation Accuracy 0.3206 | Validation F1 0.2819\n",
      "Epoch 00099 | Time(s) 0.0709 | Training Accuracy 0.4388 | Training Loss 1.3181 | Training F1 0.4175 | Validation Accuracy 0.3175 | Validation F1 0.2818\n",
      "Finished Model training and prediction\n",
      "Accuracy:  0.3271276595744681\n",
      "F1 Score:  0.2987534500351686\n",
      "Majority class baseline accuracy:  0.3155464340687532\n",
      "Saving model\n",
      "Saving model predictions for test data\n"
     ]
    }
   ],
   "source": [
    "run_inductive_model(train_and_val_df = train_and_val_df,\n",
    "                          test_df = test_df,\n",
    "                          src_dst_df = src_dst_df,\n",
    "                          model_dir = '../../../Output/Modelling/Graph Neural Network/' + model_name + '/',\n",
    "                          prediction_file_path = '../../../Data/Predictions/Graph Neural Network/' + model_name + '_predictions.xlsx',\n",
    "                          target_column = target_column,\n",
    "                          custom_mapping = custom_mapping,\n",
    "                          node_to_int = node_to_int,\n",
    "                          n_hidden = 64,\n",
    "                          n_layers = 2,\n",
    "                          dropout = 0.0,\n",
    "                          weight_decay = 5e-4,\n",
    "                          n_epochs = 100,\n",
    "                          lr = 0.01,\n",
    "                          aggregator_type = \"pool\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
