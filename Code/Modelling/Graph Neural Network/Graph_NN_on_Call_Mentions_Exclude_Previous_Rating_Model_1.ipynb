{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Graph NN on Call Mentions\n",
    "\n",
    "Considering direct mentions of companies in calls, construct a network of calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "from Graph_NN_Functions import *\n",
    "model_name = 'exclude_previous_rating_model_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Feature and Class Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>fixed_quarter_date</th>\n",
       "      <th>earnings_call_date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>rating_date</th>\n",
       "      <th>Next Rating</th>\n",
       "      <th>Next Rating Date</th>\n",
       "      <th>Previous Rating</th>\n",
       "      <th>Previous Rating Date</th>\n",
       "      <th>next_rating_date_or_end_of_data</th>\n",
       "      <th>...</th>\n",
       "      <th>Ovrst</th>\n",
       "      <th>Undrst</th>\n",
       "      <th>PN</th>\n",
       "      <th>SW</th>\n",
       "      <th>AP</th>\n",
       "      <th>OU</th>\n",
       "      <th>tone</th>\n",
       "      <th>num_q_by_len</th>\n",
       "      <th>train_test_80_20</th>\n",
       "      <th>node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2014-10-01</td>\n",
       "      <td>2014-07-22</td>\n",
       "      <td>AA</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>...</td>\n",
       "      <td>364.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>5.518519</td>\n",
       "      <td>15.261905</td>\n",
       "      <td>2.661290</td>\n",
       "      <td>2.778626</td>\n",
       "      <td>3.188264</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>train</td>\n",
       "      <td>AAPL : 2014-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>2014-10-20</td>\n",
       "      <td>AA</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>AAA</td>\n",
       "      <td>2014-04-24</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>...</td>\n",
       "      <td>465.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>5.348485</td>\n",
       "      <td>15.934783</td>\n",
       "      <td>3.296482</td>\n",
       "      <td>3.059211</td>\n",
       "      <td>3.681858</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>train</td>\n",
       "      <td>AAPL : 2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>2015-01-27</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>AA</td>\n",
       "      <td>2014-05-27</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>...</td>\n",
       "      <td>468.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>3.927711</td>\n",
       "      <td>8.113636</td>\n",
       "      <td>2.841346</td>\n",
       "      <td>3.099338</td>\n",
       "      <td>1.307366</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>train</td>\n",
       "      <td>AAPL : 2015-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>2015-04-27</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-06-02</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-05-28</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>...</td>\n",
       "      <td>415.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>9.142857</td>\n",
       "      <td>2.640187</td>\n",
       "      <td>3.074074</td>\n",
       "      <td>2.025933</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>train</td>\n",
       "      <td>AAPL : 2015-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>2015-07-21</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-08-25</td>\n",
       "      <td>AA</td>\n",
       "      <td>2016-05-20</td>\n",
       "      <td>AA</td>\n",
       "      <td>2015-06-02</td>\n",
       "      <td>2016-05-20</td>\n",
       "      <td>...</td>\n",
       "      <td>449.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>4.209877</td>\n",
       "      <td>10.442857</td>\n",
       "      <td>2.579909</td>\n",
       "      <td>3.033784</td>\n",
       "      <td>1.815531</td>\n",
       "      <td>0.003915</td>\n",
       "      <td>train</td>\n",
       "      <td>AAPL : 2015-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2015-10-01</td>\n",
       "      <td>2015-08-04</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>...</td>\n",
       "      <td>298.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>3.611650</td>\n",
       "      <td>15.634615</td>\n",
       "      <td>2.911215</td>\n",
       "      <td>2.013514</td>\n",
       "      <td>1.744657</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>train</td>\n",
       "      <td>ZTS : 2015-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5505</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-01-30</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>...</td>\n",
       "      <td>395.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>3.766917</td>\n",
       "      <td>15.848101</td>\n",
       "      <td>2.791667</td>\n",
       "      <td>1.779279</td>\n",
       "      <td>1.596294</td>\n",
       "      <td>0.003859</td>\n",
       "      <td>train</td>\n",
       "      <td>ZTS : 2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5506</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-02-16</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>...</td>\n",
       "      <td>469.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>3.565517</td>\n",
       "      <td>17.506849</td>\n",
       "      <td>2.926829</td>\n",
       "      <td>2.161290</td>\n",
       "      <td>2.287146</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>train</td>\n",
       "      <td>ZTS : 2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>2016-05-04</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>...</td>\n",
       "      <td>449.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>3.572650</td>\n",
       "      <td>15.235294</td>\n",
       "      <td>3.023715</td>\n",
       "      <td>2.088372</td>\n",
       "      <td>1.739992</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>train</td>\n",
       "      <td>ZTS : 2016-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5508</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>2016-08-03</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-01-22</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2015-11-03</td>\n",
       "      <td>2016-12-23</td>\n",
       "      <td>...</td>\n",
       "      <td>460.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2.858896</td>\n",
       "      <td>12.395349</td>\n",
       "      <td>2.840000</td>\n",
       "      <td>2.288557</td>\n",
       "      <td>0.976340</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>train</td>\n",
       "      <td>ZTS : 2016-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5509 rows × 206 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ticker fixed_quarter_date earnings_call_date Rating rating_date  \\\n",
       "0      AAPL         2014-10-01         2014-07-22     AA  2014-05-27   \n",
       "1      AAPL         2015-01-01         2014-10-20     AA  2014-05-27   \n",
       "2      AAPL         2015-04-01         2015-01-27     AA  2015-02-18   \n",
       "3      AAPL         2015-07-01         2015-04-27     AA  2015-06-02   \n",
       "4      AAPL         2015-10-01         2015-07-21     AA  2015-08-25   \n",
       "...     ...                ...                ...    ...         ...   \n",
       "5504    ZTS         2015-10-01         2015-08-04    BBB  2015-01-30   \n",
       "5505    ZTS         2016-01-01         2015-11-03    BBB  2015-11-03   \n",
       "5506    ZTS         2016-04-01         2016-02-16    BBB  2016-01-22   \n",
       "5507    ZTS         2016-07-01         2016-05-04    BBB  2016-01-22   \n",
       "5508    ZTS         2016-10-01         2016-08-03    BBB  2016-01-22   \n",
       "\n",
       "     Next Rating Next Rating Date Previous Rating Previous Rating Date  \\\n",
       "0             AA       2015-02-18             AAA           2014-04-24   \n",
       "1             AA       2015-02-18             AAA           2014-04-24   \n",
       "2             AA       2015-05-28              AA           2014-05-27   \n",
       "3             AA       2015-08-25              AA           2015-05-28   \n",
       "4             AA       2016-05-20              AA           2015-06-02   \n",
       "...          ...              ...             ...                  ...   \n",
       "5504         BBB       2015-11-03             BBB           2014-01-31   \n",
       "5505         BBB       2016-01-22             BBB           2015-01-30   \n",
       "5506         BBB       2016-12-23             BBB           2015-11-03   \n",
       "5507         BBB       2016-12-23             BBB           2015-11-03   \n",
       "5508         BBB       2016-12-23             BBB           2015-11-03   \n",
       "\n",
       "     next_rating_date_or_end_of_data  ...  Ovrst Undrst        PN         SW  \\\n",
       "0                         2015-02-18  ...  364.0  131.0  5.518519  15.261905   \n",
       "1                         2015-02-18  ...  465.0  152.0  5.348485  15.934783   \n",
       "2                         2015-05-28  ...  468.0  151.0  3.927711   8.113636   \n",
       "3                         2015-08-25  ...  415.0  135.0  5.250000   9.142857   \n",
       "4                         2016-05-20  ...  449.0  148.0  4.209877  10.442857   \n",
       "...                              ...  ...    ...    ...       ...        ...   \n",
       "5504                      2015-11-03  ...  298.0  148.0  3.611650  15.634615   \n",
       "5505                      2016-01-22  ...  395.0  222.0  3.766917  15.848101   \n",
       "5506                      2016-12-23  ...  469.0  217.0  3.565517  17.506849   \n",
       "5507                      2016-12-23  ...  449.0  215.0  3.572650  15.235294   \n",
       "5508                      2016-12-23  ...  460.0  201.0  2.858896  12.395349   \n",
       "\n",
       "            AP        OU      tone num_q_by_len train_test_80_20  \\\n",
       "0     2.661290  2.778626  3.188264     0.003822            train   \n",
       "1     3.296482  3.059211  3.681858     0.002766            train   \n",
       "2     2.841346  3.099338  1.307366     0.004628            train   \n",
       "3     2.640187  3.074074  2.025933     0.003861            train   \n",
       "4     2.579909  3.033784  1.815531     0.003915            train   \n",
       "...        ...       ...       ...          ...              ...   \n",
       "5504  2.911215  2.013514  1.744657     0.001458            train   \n",
       "5505  2.791667  1.779279  1.596294     0.003859            train   \n",
       "5506  2.926829  2.161290  2.287146     0.003928            train   \n",
       "5507  3.023715  2.088372  1.739992     0.003182            train   \n",
       "5508  2.840000  2.288557  0.976340     0.002697            train   \n",
       "\n",
       "                   node  \n",
       "0     AAPL : 2014-10-01  \n",
       "1     AAPL : 2015-01-01  \n",
       "2     AAPL : 2015-04-01  \n",
       "3     AAPL : 2015-07-01  \n",
       "4     AAPL : 2015-10-01  \n",
       "...                 ...  \n",
       "5504   ZTS : 2015-10-01  \n",
       "5505   ZTS : 2016-01-01  \n",
       "5506   ZTS : 2016-04-01  \n",
       "5507   ZTS : 2016-07-01  \n",
       "5508   ZTS : 2016-10-01  \n",
       "\n",
       "[5509 rows x 206 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load feature and class data\n",
    "feature_and_class_df = load_feature_and_class_data()\n",
    "feature_and_class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load column names\n",
    "numeric_feature_columns, cat_feature_columns, target_column, custom_mapping = get_column_names_and_mapping(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature names: \n",
      "['num__Altman_Z']\n"
     ]
    }
   ],
   "source": [
    "# Prepare matrices\n",
    "X_train_scaled, X_test_scaled, y_train, y_test, feature_names, train_ticker_by_fixed_quarter_date, test_ticker_by_fixed_quarter_date = prepare_matrices(feature_and_class_df, numeric_feature_columns, cat_feature_columns, target_column, custom_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      num__Altman_Z\n",
      "0          1.901993\n",
      "1          2.992192\n",
      "2          1.643599\n",
      "3          3.102926\n",
      "4          2.982099\n",
      "...             ...\n",
      "4386       1.261070\n",
      "4387       1.026472\n",
      "4388       0.703264\n",
      "4389       0.872260\n",
      "4390       1.026999\n",
      "\n",
      "[4391 rows x 1 columns]\n",
      "finalized dfs\n",
      "      num__Altman_Z  Rating               node\n",
      "0          1.901993       1  AAPL : 2014-10-01\n",
      "1          2.992192       1  AAPL : 2015-01-01\n",
      "2          1.643599       1  AAPL : 2015-04-01\n",
      "3          3.102926       1  AAPL : 2015-07-01\n",
      "4          2.982099       1  AAPL : 2015-10-01\n",
      "...             ...     ...                ...\n",
      "4386       1.261070       3   ZTS : 2015-10-01\n",
      "4387       1.026472       3   ZTS : 2016-01-01\n",
      "4388       0.703264       3   ZTS : 2016-04-01\n",
      "4389       0.872260       3   ZTS : 2016-07-01\n",
      "4390       1.026999       3   ZTS : 2016-10-01\n",
      "\n",
      "[4391 rows x 3 columns]\n",
      "      num__Altman_Z  Rating               node\n",
      "0          2.967006       1  AAPL : 2016-07-01\n",
      "1          0.740850       2  ABBV : 2015-04-01\n",
      "2         -0.214253       2  ABBV : 2016-04-01\n",
      "3         -0.433793       2   ABC : 2012-04-01\n",
      "4         -0.438442       2   ABC : 2013-01-01\n",
      "...             ...     ...                ...\n",
      "1113       3.545117       0   XOM : 2016-01-01\n",
      "1114       0.794257       3   YUM : 2015-04-01\n",
      "1115      -0.613242       5  ZBRA : 2016-10-01\n",
      "1116       0.364546       3   ZTS : 2013-10-01\n",
      "1117       0.447307       3   ZTS : 2014-04-01\n",
      "\n",
      "[1118 rows x 3 columns]\n",
      "missing values of target_column in train_and_val_df or test_df?\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Assemble back into dataframes\n",
    "\n",
    "# Train and val\n",
    "train_and_val_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "print(train_and_val_df)\n",
    "# Add y_train\n",
    "train_and_val_df[target_column] = y_train.reset_index(drop=True)\n",
    "# Add ticker by fixed quarter date\n",
    "train_and_val_df = pd.concat([train_ticker_by_fixed_quarter_date.reset_index(drop=True).sort_values(['ticker', 'fixed_quarter_date']), train_and_val_df], axis=1)\n",
    "# Add node by merging with feature_and_class_df (inner join)\n",
    "train_and_val_df = train_and_val_df.merge(feature_and_class_df[['ticker', 'fixed_quarter_date', 'node']], on=['ticker', 'fixed_quarter_date'], how='inner')\n",
    "# Drop ticker and fixed_quarter_date\n",
    "train_and_val_df = train_and_val_df.drop(['ticker', 'fixed_quarter_date'], axis=1)\n",
    "\n",
    "# Test\n",
    "test_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "# Add y_test\n",
    "test_df[target_column] = y_test.reset_index(drop=True)\n",
    "# Add ticker by fixed quarter date\n",
    "test_df = pd.concat([test_ticker_by_fixed_quarter_date.reset_index(drop=True).sort_values(['ticker', 'fixed_quarter_date']), test_df], axis=1)\n",
    "# Add node by merging with feature_and_class_df (inner join)\n",
    "test_df = test_df.merge(feature_and_class_df[['ticker', 'fixed_quarter_date', 'node']], on=['ticker', 'fixed_quarter_date'], how='inner')\n",
    "# Drop ticker and fixed_quarter_date\n",
    "test_df = test_df.drop(['ticker', 'fixed_quarter_date'], axis=1)\n",
    "\n",
    "print('finalized dfs')\n",
    "print(train_and_val_df)\n",
    "print(test_df)\n",
    "print('missing values of target_column in train_and_val_df or test_df?')\n",
    "print(train_and_val_df[target_column].isnull().sum() > 0)\n",
    "print(test_df[target_column].isnull().sum() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pairwise Mentions Data\n",
    "\n",
    "Note: it's OK if we lose observations here, because on some fixed quarter dates we don't have data for both companies in a mention link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num obs\n",
      "2750\n",
      "num obs\n",
      "2750\n",
      "num obs\n",
      "1807\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>dst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LPX : 2016-04-01</td>\n",
       "      <td>MCO : 2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SBAC : 2016-04-01</td>\n",
       "      <td>MCO : 2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR : 2016-04-01</td>\n",
       "      <td>MCO : 2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATVI : 2016-04-01</td>\n",
       "      <td>MCO : 2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DVN : 2016-04-01</td>\n",
       "      <td>MCO : 2016-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>THC : 2014-10-01</td>\n",
       "      <td>ACIW : 2014-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>HOV : 2013-07-01</td>\n",
       "      <td>CNSL : 2013-07-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1804</th>\n",
       "      <td>HOV : 2014-04-01</td>\n",
       "      <td>TOL : 2014-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>HOV : 2015-10-01</td>\n",
       "      <td>MTH : 2015-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>HOV : 2015-10-01</td>\n",
       "      <td>CNSL : 2015-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1807 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    src                dst\n",
       "0      LPX : 2016-04-01   MCO : 2016-04-01\n",
       "1     SBAC : 2016-04-01   MCO : 2016-04-01\n",
       "2       AR : 2016-04-01   MCO : 2016-04-01\n",
       "3     ATVI : 2016-04-01   MCO : 2016-04-01\n",
       "4      DVN : 2016-04-01   MCO : 2016-04-01\n",
       "...                 ...                ...\n",
       "1802   THC : 2014-10-01  ACIW : 2014-10-01\n",
       "1803   HOV : 2013-07-01  CNSL : 2013-07-01\n",
       "1804   HOV : 2014-04-01   TOL : 2014-04-01\n",
       "1805   HOV : 2015-10-01   MTH : 2015-10-01\n",
       "1806   HOV : 2015-10-01  CNSL : 2015-10-01\n",
       "\n",
       "[1807 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dst_df = load_src_dst_data()\n",
    "print('num obs')\n",
    "print(len(src_dst_df))\n",
    "# Convert fixed_quarter_date to a string\n",
    "src_dst_df['fixed_quarter_date'] = src_dst_df['fixed_quarter_date'].astype(str)\n",
    "feature_and_class_df['fixed_quarter_date'] = feature_and_class_df['fixed_quarter_date'].astype(str)\n",
    "# Join with feature_and_class_df to get node for src_ticker and dst_ticker\n",
    "src_dst_df = src_dst_df.merge(feature_and_class_df[['ticker', 'fixed_quarter_date', 'node']], left_on=['src_ticker', 'fixed_quarter_date'], right_on=['ticker', 'fixed_quarter_date'], how='inner').rename(columns={'node': 'src_node'})\n",
    "print('num obs')\n",
    "print(len(src_dst_df))\n",
    "src_dst_df = src_dst_df.merge(feature_and_class_df[['ticker', 'fixed_quarter_date', 'node']], left_on=['dst_ticker', 'fixed_quarter_date'], right_on=['ticker', 'fixed_quarter_date'], how='inner').rename(columns={'node': 'dst_node'})\n",
    "print('num obs')\n",
    "print(len(src_dst_df))\n",
    "# Limit columns to just src_node and dst_node, rename to src and dst\n",
    "src_dst_df = src_dst_df[['src_node', 'dst_node']].rename(columns={'src_node': 'src', 'dst_node': 'dst'})\n",
    "src_dst_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edits to train and val and test dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keeping only items that are connected/have a node in src or dst in src_dst_df\n",
      "drop items in classes with only one node\n",
      "length of train_and_val_df\n",
      "1563\n",
      "new length of train_and_val_df\n",
      "1562\n",
      "keeping only src and dst that are in train_and_val_df or test_df\n",
      "length of src_dst_df\n",
      "1807\n",
      "new length of src_dst_df\n",
      "1806\n",
      "keeping only items that are connected/have a node in src or dst in src_dst_df again\n",
      "length of train_and_val_df\n",
      "1562\n",
      "length of test_df\n",
      "402\n",
      "new length of train_and_val_df\n",
      "1562\n",
      "new length of test_df\n",
      "402\n"
     ]
    }
   ],
   "source": [
    "# Limit train_and_val_df and test_df to just items with a node in one of the src or dst columns\n",
    "print('keeping only items that are connected/have a node in src or dst in src_dst_df')\n",
    "train_and_val_df = train_and_val_df[train_and_val_df['node'].isin(src_dst_df['src']) | train_and_val_df['node'].isin(src_dst_df['dst'])]\n",
    "test_df = test_df[test_df['node'].isin(src_dst_df['src']) | test_df['node'].isin(src_dst_df['dst'])]\n",
    "\n",
    "# Drop any items that belong to target_column values with only one node\n",
    "print('drop items in classes with only one node')\n",
    "print('length of train_and_val_df')\n",
    "print(len(train_and_val_df))\n",
    "train_and_val_df = train_and_val_df.groupby(target_column).filter(lambda x: len(x) > 1).reset_index(drop=True)\n",
    "print('new length of train_and_val_df')\n",
    "print(len(train_and_val_df))\n",
    "\n",
    "# Limit src and dst to just nodes in train_and_val_df or test_df\n",
    "print('keeping only src and dst that are in train_and_val_df or test_df')\n",
    "print('length of src_dst_df')\n",
    "print(len(src_dst_df))\n",
    "src_dst_df = src_dst_df[src_dst_df['src'].isin(train_and_val_df['node']) | src_dst_df['src'].isin(test_df['node'])]\n",
    "src_dst_df = src_dst_df[src_dst_df['dst'].isin(train_and_val_df['node']) | src_dst_df['dst'].isin(test_df['node'])]\n",
    "print('new length of src_dst_df')\n",
    "print(len(src_dst_df))\n",
    "\n",
    "# Limit train_and_val_df and test_df to just items with a node in one of the src or dst columns\n",
    "print('keeping only items that are connected/have a node in src or dst in src_dst_df again')\n",
    "print('length of train_and_val_df')\n",
    "print(len(train_and_val_df))\n",
    "print('length of test_df')\n",
    "print(len(test_df))\n",
    "train_and_val_df = train_and_val_df[train_and_val_df['node'].isin(src_dst_df['src']) | train_and_val_df['node'].isin(src_dst_df['dst'])]\n",
    "test_df = test_df[test_df['node'].isin(src_dst_df['src']) | test_df['node'].isin(src_dst_df['dst'])]\n",
    "print('new length of train_and_val_df')\n",
    "print(len(train_and_val_df))\n",
    "print('new length of test_df')\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Node as Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all values of node in train_and_val_df and test_df\n",
    "all_nodes = list(set(train_and_val_df['node']) | set(test_df['node']))\n",
    "# Encode as integers, create a mapping\n",
    "node_to_int = {node: i for i, node in enumerate(all_nodes)}\n",
    "# Add to train_and_val_df and test_df as a replacement of node\n",
    "train_and_val_df['node'] = train_and_val_df['node'].map(node_to_int)\n",
    "test_df['node'] = test_df['node'].map(node_to_int)\n",
    "# Same for src and dst\n",
    "src_dst_df['src'] = src_dst_df['src'].map(node_to_int)\n",
    "src_dst_df['dst'] = src_dst_df['dst'].map(node_to_int)\n",
    "# Convert dictionary of node_to_int to df\n",
    "node_to_int_df = pd.DataFrame(list(node_to_int.items()), columns=['node', 'node_int'])\n",
    "# Save to disk\n",
    "node_to_int_df.to_excel('../../../Output/Modelling/Graph Neural Network/' + model_name + '/' + 'node_to_int.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Inductive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Further slice the train dataset into train and validation datasets.\n",
      "The training data has shape: (1249, 3).\n",
      "The validation data has shape: (313, 3).\n",
      "The test data has shape: (402, 3).\n",
      "Generate train, validation, and test masks.\n",
      "sum of train mask\n",
      "tensor(1249)\n",
      "sum of val mask\n",
      "tensor(313)\n",
      "sum of test mask\n",
      "tensor(402)\n",
      "Number of nodes = 1964\n",
      "Number of features for each node = 1\n",
      "Number of classes = 9.\n",
      "Initializing Model\n",
      "Initialized Model\n",
      "NodeClassification(\n",
      "  (gconv_model): GraphSAGEModel(\n",
      "    (layers): ModuleList(\n",
      "      (0): SAGEConv(\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (fc_pool): Linear(in_features=1, out_features=1, bias=True)\n",
      "        (fc_neigh): Linear(in_features=1, out_features=32, bias=False)\n",
      "        (fc_self): Linear(in_features=1, out_features=32, bias=True)\n",
      "      )\n",
      "      (1): SAGEConv(\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (fc_pool): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (fc_neigh): Linear(in_features=32, out_features=32, bias=False)\n",
      "        (fc_self): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (2): SAGEConv(\n",
      "        (feat_drop): Dropout(p=0.0, inplace=False)\n",
      "        (fc_pool): Linear(in_features=32, out_features=32, bias=True)\n",
      "        (fc_neigh): Linear(in_features=32, out_features=9, bias=False)\n",
      "        (fc_self): Linear(in_features=32, out_features=9, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loss_fcn): CrossEntropyLoss()\n",
      ")\n",
      "<generator object Module.parameters at 0x0000021CB4B89E00>\n",
      "GPU available:  False\n",
      "Starting Model training and prediction\n",
      "Epoch 00000 | Time(s) 0.0346 | Training Accuracy 0.2178 | Training Loss 2.8626 | Training F1 0.2136 | Validation Accuracy 0.2332 | Validation F1 0.2280\n",
      "Epoch 00001 | Time(s) 0.0331 | Training Accuracy 0.2946 | Training Loss 2.1339 | Training F1 0.2442 | Validation Accuracy 0.3035 | Validation F1 0.2585\n",
      "Epoch 00002 | Time(s) 0.0498 | Training Accuracy 0.3155 | Training Loss 1.8798 | Training F1 0.2127 | Validation Accuracy 0.3387 | Validation F1 0.2248\n",
      "Epoch 00003 | Time(s) 0.0475 | Training Accuracy 0.3275 | Training Loss 1.7824 | Training F1 0.2145 | Validation Accuracy 0.3450 | Validation F1 0.2289\n",
      "Epoch 00004 | Time(s) 0.0450 | Training Accuracy 0.3363 | Training Loss 1.7172 | Training F1 0.2238 | Validation Accuracy 0.3227 | Validation F1 0.2131\n",
      "Epoch 00005 | Time(s) 0.0428 | Training Accuracy 0.3459 | Training Loss 1.6587 | Training F1 0.2889 | Validation Accuracy 0.3163 | Validation F1 0.2497\n",
      "Epoch 00006 | Time(s) 0.0414 | Training Accuracy 0.3291 | Training Loss 1.6326 | Training F1 0.2906 | Validation Accuracy 0.3323 | Validation F1 0.2931\n",
      "Epoch 00007 | Time(s) 0.0397 | Training Accuracy 0.3235 | Training Loss 1.6226 | Training F1 0.2777 | Validation Accuracy 0.3195 | Validation F1 0.2723\n",
      "Epoch 00008 | Time(s) 0.0375 | Training Accuracy 0.3467 | Training Loss 1.5967 | Training F1 0.2735 | Validation Accuracy 0.3035 | Validation F1 0.2235\n",
      "Epoch 00009 | Time(s) 0.0370 | Training Accuracy 0.3475 | Training Loss 1.5663 | Training F1 0.2551 | Validation Accuracy 0.3163 | Validation F1 0.2171\n",
      "Epoch 00010 | Time(s) 0.0368 | Training Accuracy 0.3443 | Training Loss 1.5534 | Training F1 0.2536 | Validation Accuracy 0.3163 | Validation F1 0.2179\n",
      "Epoch 00011 | Time(s) 0.0364 | Training Accuracy 0.3355 | Training Loss 1.5538 | Training F1 0.2519 | Validation Accuracy 0.3259 | Validation F1 0.2353\n",
      "Epoch 00012 | Time(s) 0.0362 | Training Accuracy 0.3467 | Training Loss 1.5564 | Training F1 0.3215 | Validation Accuracy 0.3450 | Validation F1 0.3195\n",
      "Epoch 00013 | Time(s) 0.0359 | Training Accuracy 0.3403 | Training Loss 1.5523 | Training F1 0.3152 | Validation Accuracy 0.3259 | Validation F1 0.2882\n",
      "Epoch 00014 | Time(s) 0.0359 | Training Accuracy 0.3491 | Training Loss 1.5438 | Training F1 0.3175 | Validation Accuracy 0.3578 | Validation F1 0.3070\n",
      "Epoch 00015 | Time(s) 0.0358 | Training Accuracy 0.3571 | Training Loss 1.5381 | Training F1 0.3233 | Validation Accuracy 0.3514 | Validation F1 0.3063\n",
      "Epoch 00016 | Time(s) 0.0357 | Training Accuracy 0.3603 | Training Loss 1.5327 | Training F1 0.3277 | Validation Accuracy 0.3578 | Validation F1 0.3136\n",
      "Epoch 00017 | Time(s) 0.0356 | Training Accuracy 0.3707 | Training Loss 1.5238 | Training F1 0.3420 | Validation Accuracy 0.3610 | Validation F1 0.3270\n",
      "Epoch 00018 | Time(s) 0.0353 | Training Accuracy 0.3771 | Training Loss 1.5163 | Training F1 0.3431 | Validation Accuracy 0.3227 | Validation F1 0.2956\n",
      "Epoch 00019 | Time(s) 0.0352 | Training Accuracy 0.3667 | Training Loss 1.5143 | Training F1 0.3244 | Validation Accuracy 0.3323 | Validation F1 0.3029\n",
      "Epoch 00020 | Time(s) 0.0352 | Training Accuracy 0.3715 | Training Loss 1.5135 | Training F1 0.3247 | Validation Accuracy 0.3323 | Validation F1 0.3007\n",
      "Epoch 00021 | Time(s) 0.0350 | Training Accuracy 0.3739 | Training Loss 1.5099 | Training F1 0.3307 | Validation Accuracy 0.3291 | Validation F1 0.2985\n",
      "Epoch 00022 | Time(s) 0.0349 | Training Accuracy 0.3819 | Training Loss 1.5061 | Training F1 0.3443 | Validation Accuracy 0.3674 | Validation F1 0.3331\n",
      "Epoch 00023 | Time(s) 0.0353 | Training Accuracy 0.3779 | Training Loss 1.5043 | Training F1 0.3426 | Validation Accuracy 0.3642 | Validation F1 0.3256\n",
      "Epoch 00024 | Time(s) 0.0348 | Training Accuracy 0.3819 | Training Loss 1.5034 | Training F1 0.3475 | Validation Accuracy 0.3706 | Validation F1 0.3290\n",
      "Epoch 00025 | Time(s) 0.0348 | Training Accuracy 0.3859 | Training Loss 1.5012 | Training F1 0.3525 | Validation Accuracy 0.3642 | Validation F1 0.3304\n",
      "Epoch 00026 | Time(s) 0.0349 | Training Accuracy 0.3803 | Training Loss 1.4983 | Training F1 0.3454 | Validation Accuracy 0.3482 | Validation F1 0.3169\n",
      "Epoch 00027 | Time(s) 0.0350 | Training Accuracy 0.3771 | Training Loss 1.4973 | Training F1 0.3360 | Validation Accuracy 0.3450 | Validation F1 0.3102\n",
      "Epoch 00028 | Time(s) 0.0352 | Training Accuracy 0.3659 | Training Loss 1.4973 | Training F1 0.3185 | Validation Accuracy 0.3387 | Validation F1 0.3001\n",
      "Epoch 00029 | Time(s) 0.0353 | Training Accuracy 0.3643 | Training Loss 1.4959 | Training F1 0.3146 | Validation Accuracy 0.3387 | Validation F1 0.2992\n",
      "Epoch 00030 | Time(s) 0.0350 | Training Accuracy 0.3683 | Training Loss 1.4931 | Training F1 0.3191 | Validation Accuracy 0.3387 | Validation F1 0.3014\n",
      "Epoch 00031 | Time(s) 0.0350 | Training Accuracy 0.3803 | Training Loss 1.4907 | Training F1 0.3378 | Validation Accuracy 0.3419 | Validation F1 0.3087\n",
      "Epoch 00032 | Time(s) 0.0350 | Training Accuracy 0.3771 | Training Loss 1.4892 | Training F1 0.3369 | Validation Accuracy 0.3323 | Validation F1 0.3034\n",
      "Epoch 00033 | Time(s) 0.0349 | Training Accuracy 0.3795 | Training Loss 1.4881 | Training F1 0.3410 | Validation Accuracy 0.3387 | Validation F1 0.3105\n",
      "Epoch 00034 | Time(s) 0.0347 | Training Accuracy 0.3755 | Training Loss 1.4868 | Training F1 0.3364 | Validation Accuracy 0.3259 | Validation F1 0.2982\n",
      "Epoch 00035 | Time(s) 0.0349 | Training Accuracy 0.3787 | Training Loss 1.4854 | Training F1 0.3368 | Validation Accuracy 0.3323 | Validation F1 0.3024\n",
      "Epoch 00036 | Time(s) 0.0349 | Training Accuracy 0.3755 | Training Loss 1.4842 | Training F1 0.3313 | Validation Accuracy 0.3387 | Validation F1 0.3069\n",
      "Epoch 00037 | Time(s) 0.0349 | Training Accuracy 0.3715 | Training Loss 1.4831 | Training F1 0.3287 | Validation Accuracy 0.3450 | Validation F1 0.3147\n",
      "Epoch 00038 | Time(s) 0.0348 | Training Accuracy 0.3675 | Training Loss 1.4818 | Training F1 0.3277 | Validation Accuracy 0.3387 | Validation F1 0.3105\n",
      "Epoch 00039 | Time(s) 0.0347 | Training Accuracy 0.3763 | Training Loss 1.4805 | Training F1 0.3412 | Validation Accuracy 0.3450 | Validation F1 0.3152\n",
      "Epoch 00040 | Time(s) 0.0347 | Training Accuracy 0.3843 | Training Loss 1.4794 | Training F1 0.3515 | Validation Accuracy 0.3450 | Validation F1 0.3190\n",
      "Epoch 00041 | Time(s) 0.0347 | Training Accuracy 0.3795 | Training Loss 1.4783 | Training F1 0.3470 | Validation Accuracy 0.3419 | Validation F1 0.3159\n",
      "Epoch 00042 | Time(s) 0.0347 | Training Accuracy 0.3811 | Training Loss 1.4773 | Training F1 0.3454 | Validation Accuracy 0.3387 | Validation F1 0.3132\n",
      "Epoch 00043 | Time(s) 0.0346 | Training Accuracy 0.3851 | Training Loss 1.4762 | Training F1 0.3447 | Validation Accuracy 0.3291 | Validation F1 0.3002\n",
      "Epoch 00044 | Time(s) 0.0346 | Training Accuracy 0.3755 | Training Loss 1.4752 | Training F1 0.3299 | Validation Accuracy 0.3259 | Validation F1 0.2934\n",
      "Epoch 00045 | Time(s) 0.0347 | Training Accuracy 0.3787 | Training Loss 1.4743 | Training F1 0.3329 | Validation Accuracy 0.3163 | Validation F1 0.2828\n",
      "Epoch 00046 | Time(s) 0.0348 | Training Accuracy 0.3787 | Training Loss 1.4735 | Training F1 0.3356 | Validation Accuracy 0.3195 | Validation F1 0.2874\n",
      "Epoch 00047 | Time(s) 0.0349 | Training Accuracy 0.3795 | Training Loss 1.4727 | Training F1 0.3381 | Validation Accuracy 0.3355 | Validation F1 0.3053\n",
      "Epoch 00048 | Time(s) 0.0350 | Training Accuracy 0.3867 | Training Loss 1.4718 | Training F1 0.3468 | Validation Accuracy 0.3323 | Validation F1 0.3029\n",
      "Epoch 00049 | Time(s) 0.0349 | Training Accuracy 0.3883 | Training Loss 1.4708 | Training F1 0.3505 | Validation Accuracy 0.3323 | Validation F1 0.3029\n",
      "Epoch 00050 | Time(s) 0.0350 | Training Accuracy 0.3827 | Training Loss 1.4699 | Training F1 0.3463 | Validation Accuracy 0.3291 | Validation F1 0.3005\n",
      "Epoch 00051 | Time(s) 0.0351 | Training Accuracy 0.3859 | Training Loss 1.4691 | Training F1 0.3492 | Validation Accuracy 0.3323 | Validation F1 0.3027\n",
      "Epoch 00052 | Time(s) 0.0351 | Training Accuracy 0.3883 | Training Loss 1.4683 | Training F1 0.3503 | Validation Accuracy 0.3291 | Validation F1 0.3001\n",
      "Epoch 00053 | Time(s) 0.0350 | Training Accuracy 0.3915 | Training Loss 1.4675 | Training F1 0.3550 | Validation Accuracy 0.3291 | Validation F1 0.3016\n",
      "Epoch 00054 | Time(s) 0.0350 | Training Accuracy 0.3899 | Training Loss 1.4666 | Training F1 0.3544 | Validation Accuracy 0.3291 | Validation F1 0.3019\n",
      "Epoch 00055 | Time(s) 0.0349 | Training Accuracy 0.3883 | Training Loss 1.4656 | Training F1 0.3524 | Validation Accuracy 0.3291 | Validation F1 0.3029\n",
      "Epoch 00056 | Time(s) 0.0349 | Training Accuracy 0.3931 | Training Loss 1.4647 | Training F1 0.3558 | Validation Accuracy 0.3387 | Validation F1 0.3097\n",
      "Epoch 00057 | Time(s) 0.0349 | Training Accuracy 0.3987 | Training Loss 1.4638 | Training F1 0.3605 | Validation Accuracy 0.3355 | Validation F1 0.3062\n",
      "Epoch 00058 | Time(s) 0.0348 | Training Accuracy 0.3987 | Training Loss 1.4629 | Training F1 0.3615 | Validation Accuracy 0.3387 | Validation F1 0.3112\n",
      "Epoch 00059 | Time(s) 0.0348 | Training Accuracy 0.3955 | Training Loss 1.4621 | Training F1 0.3582 | Validation Accuracy 0.3355 | Validation F1 0.3088\n",
      "Epoch 00060 | Time(s) 0.0349 | Training Accuracy 0.3931 | Training Loss 1.4611 | Training F1 0.3559 | Validation Accuracy 0.3291 | Validation F1 0.3034\n",
      "Epoch 00061 | Time(s) 0.0348 | Training Accuracy 0.3963 | Training Loss 1.4600 | Training F1 0.3581 | Validation Accuracy 0.3323 | Validation F1 0.3049\n",
      "Epoch 00062 | Time(s) 0.0347 | Training Accuracy 0.3971 | Training Loss 1.4587 | Training F1 0.3597 | Validation Accuracy 0.3355 | Validation F1 0.3069\n",
      "Epoch 00063 | Time(s) 0.0347 | Training Accuracy 0.3963 | Training Loss 1.4575 | Training F1 0.3597 | Validation Accuracy 0.3355 | Validation F1 0.3084\n",
      "Epoch 00064 | Time(s) 0.0347 | Training Accuracy 0.3915 | Training Loss 1.4561 | Training F1 0.3561 | Validation Accuracy 0.3163 | Validation F1 0.2925\n",
      "Epoch 00065 | Time(s) 0.0347 | Training Accuracy 0.3923 | Training Loss 1.4550 | Training F1 0.3559 | Validation Accuracy 0.3259 | Validation F1 0.2986\n",
      "Epoch 00066 | Time(s) 0.0349 | Training Accuracy 0.3891 | Training Loss 1.4534 | Training F1 0.3515 | Validation Accuracy 0.3227 | Validation F1 0.2953\n",
      "Epoch 00067 | Time(s) 0.0349 | Training Accuracy 0.3915 | Training Loss 1.4519 | Training F1 0.3517 | Validation Accuracy 0.3259 | Validation F1 0.2960\n",
      "Epoch 00068 | Time(s) 0.0348 | Training Accuracy 0.3907 | Training Loss 1.4503 | Training F1 0.3522 | Validation Accuracy 0.3259 | Validation F1 0.2966\n",
      "Epoch 00069 | Time(s) 0.0346 | Training Accuracy 0.3947 | Training Loss 1.4488 | Training F1 0.3582 | Validation Accuracy 0.3259 | Validation F1 0.2973\n",
      "Epoch 00070 | Time(s) 0.0347 | Training Accuracy 0.3955 | Training Loss 1.4472 | Training F1 0.3579 | Validation Accuracy 0.3323 | Validation F1 0.3029\n",
      "Epoch 00071 | Time(s) 0.0346 | Training Accuracy 0.3963 | Training Loss 1.4456 | Training F1 0.3604 | Validation Accuracy 0.3291 | Validation F1 0.3015\n",
      "Epoch 00072 | Time(s) 0.0346 | Training Accuracy 0.3987 | Training Loss 1.4441 | Training F1 0.3636 | Validation Accuracy 0.3323 | Validation F1 0.3049\n",
      "Epoch 00073 | Time(s) 0.0345 | Training Accuracy 0.3995 | Training Loss 1.4425 | Training F1 0.3671 | Validation Accuracy 0.3291 | Validation F1 0.3031\n",
      "Epoch 00074 | Time(s) 0.0346 | Training Accuracy 0.3987 | Training Loss 1.4409 | Training F1 0.3648 | Validation Accuracy 0.3195 | Validation F1 0.2948\n",
      "Epoch 00075 | Time(s) 0.0345 | Training Accuracy 0.3931 | Training Loss 1.4393 | Training F1 0.3569 | Validation Accuracy 0.3259 | Validation F1 0.2963\n",
      "Epoch 00076 | Time(s) 0.0345 | Training Accuracy 0.4019 | Training Loss 1.4383 | Training F1 0.3662 | Validation Accuracy 0.3227 | Validation F1 0.2955\n",
      "Epoch 00077 | Time(s) 0.0342 | Training Accuracy 0.3923 | Training Loss 1.4372 | Training F1 0.3568 | Validation Accuracy 0.3227 | Validation F1 0.2960\n",
      "Epoch 00078 | Time(s) 0.0342 | Training Accuracy 0.3947 | Training Loss 1.4345 | Training F1 0.3568 | Validation Accuracy 0.3227 | Validation F1 0.2939\n",
      "Epoch 00079 | Time(s) 0.0342 | Training Accuracy 0.3947 | Training Loss 1.4333 | Training F1 0.3575 | Validation Accuracy 0.3355 | Validation F1 0.3066\n",
      "Epoch 00080 | Time(s) 0.0343 | Training Accuracy 0.3939 | Training Loss 1.4321 | Training F1 0.3569 | Validation Accuracy 0.3195 | Validation F1 0.2926\n",
      "Epoch 00081 | Time(s) 0.0342 | Training Accuracy 0.3939 | Training Loss 1.4297 | Training F1 0.3558 | Validation Accuracy 0.3227 | Validation F1 0.2956\n",
      "Epoch 00082 | Time(s) 0.0342 | Training Accuracy 0.3947 | Training Loss 1.4288 | Training F1 0.3593 | Validation Accuracy 0.3291 | Validation F1 0.2998\n",
      "Epoch 00083 | Time(s) 0.0342 | Training Accuracy 0.3947 | Training Loss 1.4272 | Training F1 0.3589 | Validation Accuracy 0.3195 | Validation F1 0.2920\n",
      "Epoch 00084 | Time(s) 0.0341 | Training Accuracy 0.3955 | Training Loss 1.4253 | Training F1 0.3601 | Validation Accuracy 0.3227 | Validation F1 0.2954\n",
      "Epoch 00085 | Time(s) 0.0341 | Training Accuracy 0.3971 | Training Loss 1.4242 | Training F1 0.3635 | Validation Accuracy 0.3259 | Validation F1 0.2988\n",
      "Epoch 00086 | Time(s) 0.0342 | Training Accuracy 0.3995 | Training Loss 1.4225 | Training F1 0.3624 | Validation Accuracy 0.3227 | Validation F1 0.2931\n",
      "Epoch 00087 | Time(s) 0.0341 | Training Accuracy 0.3995 | Training Loss 1.4208 | Training F1 0.3626 | Validation Accuracy 0.3163 | Validation F1 0.2886\n",
      "Epoch 00088 | Time(s) 0.0342 | Training Accuracy 0.4011 | Training Loss 1.4194 | Training F1 0.3664 | Validation Accuracy 0.3227 | Validation F1 0.2961\n",
      "Epoch 00089 | Time(s) 0.0343 | Training Accuracy 0.4035 | Training Loss 1.4178 | Training F1 0.3646 | Validation Accuracy 0.3227 | Validation F1 0.2930\n",
      "Epoch 00090 | Time(s) 0.0344 | Training Accuracy 0.4043 | Training Loss 1.4161 | Training F1 0.3685 | Validation Accuracy 0.3163 | Validation F1 0.2891\n",
      "Epoch 00091 | Time(s) 0.0343 | Training Accuracy 0.4035 | Training Loss 1.4145 | Training F1 0.3691 | Validation Accuracy 0.3195 | Validation F1 0.2929\n",
      "Epoch 00092 | Time(s) 0.0343 | Training Accuracy 0.4059 | Training Loss 1.4133 | Training F1 0.3692 | Validation Accuracy 0.3227 | Validation F1 0.2929\n",
      "Epoch 00093 | Time(s) 0.0344 | Training Accuracy 0.4043 | Training Loss 1.4118 | Training F1 0.3701 | Validation Accuracy 0.3291 | Validation F1 0.3021\n",
      "Epoch 00094 | Time(s) 0.0348 | Training Accuracy 0.4035 | Training Loss 1.4101 | Training F1 0.3678 | Validation Accuracy 0.3227 | Validation F1 0.2946\n",
      "Epoch 00095 | Time(s) 0.0348 | Training Accuracy 0.4051 | Training Loss 1.4086 | Training F1 0.3706 | Validation Accuracy 0.3259 | Validation F1 0.2971\n",
      "Epoch 00096 | Time(s) 0.0348 | Training Accuracy 0.4067 | Training Loss 1.4069 | Training F1 0.3701 | Validation Accuracy 0.3163 | Validation F1 0.2865\n",
      "Epoch 00097 | Time(s) 0.0348 | Training Accuracy 0.4051 | Training Loss 1.4053 | Training F1 0.3703 | Validation Accuracy 0.3291 | Validation F1 0.2995\n",
      "Epoch 00098 | Time(s) 0.0347 | Training Accuracy 0.4067 | Training Loss 1.4040 | Training F1 0.3727 | Validation Accuracy 0.3195 | Validation F1 0.2913\n",
      "Epoch 00099 | Time(s) 0.0348 | Training Accuracy 0.4019 | Training Loss 1.4026 | Training F1 0.3650 | Validation Accuracy 0.3259 | Validation F1 0.2918\n",
      "Finished Model training and prediction\n",
      "Accuracy:  0.35323383084577115\n",
      "F1 Score:  0.29942506799777785\n",
      "Majority class baseline accuracy:  0.3167006109979633\n",
      "Saving model\n",
      "Saving model predictions for test data\n"
     ]
    }
   ],
   "source": [
    "run_model(train_and_val_df = train_and_val_df,\n",
    "         test_df = test_df,\n",
    "         src_dst_df = src_dst_df,\n",
    "         model_dir = '../../../Output/Modelling/Graph Neural Network/' + model_name + '/',\n",
    "         prediction_file_path = '../../../Data/Predictions/Graph Neural Network/' + model_name + '_predictions.xlsx',\n",
    "         target_column = target_column,\n",
    "         custom_mapping = custom_mapping,\n",
    "         node_to_int = node_to_int,\n",
    "         n_hidden = 32,\n",
    "         n_layers = 2,\n",
    "         dropout = 0.0,\n",
    "         weight_decay = 5e-4,\n",
    "         n_epochs = 100,\n",
    "         lr = 0.01,\n",
    "         aggregator_type = \"pool\",\n",
    "         inductive = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
