{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of Rating Models and Most Complex Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['rating_model_1', 'rating_model_2', 'rating_model_3', 'rating_model_4']\n",
    "clean_model_names = ['Rating Model 1', 'Rating Model 2', 'Rating Model 3', 'Rating Model 4']\n",
    "# set most_complex_model \n",
    "most_complex_model = 'rating_model_4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_comparison_row(model_name, clean_model_name):\n",
    "    '''\n",
    "    Given the model name and clean model name, this function returns the model comparison row.\n",
    "    '''\n",
    "\n",
    "    # Load close_exact_dict\n",
    "    close_exact_dict = pd.read_pickle('../../../../Output/Modelling/Logistic Regression/' + model_name + '/' + model_name + '_close_exact_dict.pkl')\n",
    "    # Version with each item rounded to 2 decimal places\n",
    "    close_exact_dict_rounded = {k: round(v, 2) for k, v in close_exact_dict.items()}\n",
    "    # Unpack\n",
    "    exact_predictions_share = close_exact_dict_rounded['exact_predictions_share']\n",
    "    close_predictions_share = close_exact_dict_rounded['close_predictions_share']\n",
    "\n",
    "    # Load acc_f1_majority\n",
    "    acc_f1_majority = pd.read_pickle('../../../../Output/Modelling/Logistic Regression/' + model_name + '/' + model_name + '_acc_f1_majority.pkl')\n",
    "    # Version with each item rounded to 2 decimal places\n",
    "    acc_f1_majority_rounded = {k: round(v, 2) for k, v in acc_f1_majority.items()}\n",
    "    # Unpack\n",
    "    accuracy = acc_f1_majority_rounded['accuracy']\n",
    "    f1 = acc_f1_majority_rounded['f1_score']\n",
    "    majority_baseline = acc_f1_majority_rounded['majority_baseline']\n",
    "\n",
    "    # Check exact_predictions_share == accuracy\n",
    "    print('exact predictions share == accuracy:', exact_predictions_share == accuracy)\n",
    "\n",
    "    # Get weighted average precision and recall from classification report\n",
    "    classification_report = pd.read_pickle('../../../../Output/Modelling/Logistic Regression/' + model_name + '/' + model_name + '_classification_report.pkl')\n",
    "    # Convert classification report string to dataframe\n",
    "    classification_report_lines = classification_report.split('\\n')\n",
    "    # split on spaces within and drop blanks\n",
    "    classification_report_data = [line.split() for line in classification_report_lines if line]\n",
    "    # drop lists begining with 'precision', 'accuracy', 'macro', 'weighted'\n",
    "    classification_report_data = [line for line in classification_report_data if line[0] in ['weighted']]\n",
    "    # Unpack\n",
    "    weighted_avg_precision = classification_report_data[0][2]\n",
    "    weighted_avg_recall = classification_report_data[0][3]\n",
    "\n",
    "    # Create dataframe row\n",
    "    model_comparison_row = pd.DataFrame({\n",
    "        'Model/Baseline': [clean_model_name],\n",
    "        'Accuracy': [accuracy],\n",
    "        'Weighted Average Precision': [weighted_avg_precision],\n",
    "        'Weighted Average Recall': [weighted_avg_recall],\n",
    "        'F1 Score': [f1],\n",
    "        'Share 1 Rating Or Less From Actual': [close_predictions_share]\n",
    "    })\n",
    "\n",
    "    # Return row\n",
    "    return model_comparison_row, majority_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact predictions share == accuracy: True\n",
      "exact predictions share == accuracy: True\n",
      "exact predictions share == accuracy: True\n",
      "exact predictions share == accuracy: True\n",
      "Majority baselines are the same: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model/Baseline</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weighted Average Precision</th>\n",
       "      <th>Weighted Average Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Share 1 Rating Or Less From Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rating Model 1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rating Model 2</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rating Model 3</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rating Model 4</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Majority Baseline</td>\n",
       "      <td>0.32</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model/Baseline Accuracy Weighted Average Precision  \\\n",
       "0     Rating Model 1     0.36                       0.30   \n",
       "0     Rating Model 2     0.51                       0.49   \n",
       "0     Rating Model 3     0.95                       0.95   \n",
       "0     Rating Model 4     0.95                       0.95   \n",
       "0  Majority Baseline     0.32                              \n",
       "\n",
       "  Weighted Average Recall F1 Score Share 1 Rating Or Less From Actual  \n",
       "0                    0.36     0.26                               0.82  \n",
       "0                    0.51     0.46                               0.89  \n",
       "0                    0.95     0.95                               0.99  \n",
       "0                    0.95     0.95                               0.99  \n",
       "0                                                                      "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of df rows\n",
    "model_comparison_rows = []\n",
    "majority_baselines = []\n",
    "for model_name, clean_model_name in zip(model_names, clean_model_names):\n",
    "    model_comparison_row, majority_baseline = get_model_comparison_row(model_name, clean_model_name)\n",
    "    model_comparison_rows.append(model_comparison_row)\n",
    "    majority_baselines.append(majority_baseline)\n",
    "\n",
    "# Concatenate rows\n",
    "model_comparison_df = pd.concat(model_comparison_rows)\n",
    "\n",
    "# Check majority baselines are the same\n",
    "print('Majority baselines are the same:', all([majority_baseline == majority_baselines[0] for majority_baseline in majority_baselines]))\n",
    "# Add row with Model/Baseline = 'Majority Baseline' and Accuracy = majority_baseline[0]\n",
    "model_comparison_df = pd.concat([model_comparison_df, pd.DataFrame({\n",
    "    'Model/Baseline': ['Majority Baseline'],\n",
    "    'Accuracy': [majority_baselines[0]],\n",
    "    'Weighted Average Precision': [''],\n",
    "    'Weighted Average Recall': [''],\n",
    "    'F1 Score': [''],\n",
    "    'Share 1 Rating Or Less From Actual': ['']\n",
    "})])\n",
    "\n",
    "# Export to Excel\n",
    "model_comparison_df.to_excel('../../../../Output/Modelling/Logistic Regression/Tables/model_comparison_df.xlsx', index = False)\n",
    "\n",
    "# Export to Latex\n",
    "#model_comparison_df.to_latex('../../../../Output/Modelling/Logistic Regression/Tables/model_comparison_df.tex', index = False)\n",
    "\n",
    "# Export to LaTeX\n",
    "# Format columns\n",
    "for col in model_comparison_df.columns:\n",
    "    model_comparison_df[col] = model_comparison_df[col].apply(lambda x: f'{x:.2f}' if isinstance(x, float) else x)\n",
    "# Center all columns\n",
    "lt_string = model_comparison_df.to_latex(index=False, column_format='x{0.75cm}|x{0.75cm}|x{0.75cm}|x{0.75cm}|x{0.75cm}|x{0.75cm}', escape=False)\n",
    "latex_with_font_size = \"\\\\small\\n\" + lt_string + \"\\n\\\\normalsize\"\n",
    "with open('../../../../Output/Modelling/Logistic Regression/Tables/model_comparison_df.tex', 'w') as f:\n",
    "    f.write(latex_with_font_size)\n",
    "\n",
    "model_comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Complex Classification Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AAA       0.80      0.84      0.82        19\n",
      "          AA       0.86      0.88      0.87        43\n",
      "           A       0.93      0.92      0.92       219\n",
      "         BBB       0.96      0.97      0.97       356\n",
      "          BB       0.98      0.98      0.98       313\n",
      "           B       0.97      0.95      0.96       144\n",
      "         CCC       0.93      0.93      0.93        27\n",
      "          CC       0.50      1.00      0.67         1\n",
      "           C       1.00      0.67      0.80         3\n",
      "           D       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.95      1127\n",
      "   macro avg       0.89      0.91      0.89      1127\n",
      "weighted avg       0.95      0.95      0.95      1127\n",
      "\n",
      "  Rating Precision Recall F1-Score Support\n",
      "0    AAA      0.80   0.84     0.82      19\n",
      "1     AA      0.86   0.88     0.87      43\n",
      "2      A      0.93   0.92     0.92     219\n",
      "3    BBB      0.96   0.97     0.97     356\n",
      "4     BB      0.98   0.98     0.98     313\n",
      "5      B      0.97   0.95     0.96     144\n",
      "6    CCC      0.93   0.93     0.93      27\n",
      "7     CC      0.50   1.00     0.67       1\n",
      "8      C      1.00   0.67     0.80       3\n",
      "9      D      1.00   1.00     1.00       2\n"
     ]
    }
   ],
   "source": [
    "# Load classificiation report from pickle\n",
    "classification_report = pd.read_pickle('../../../../Output/Modelling/Logistic Regression/' + most_complex_model + '/' + most_complex_model + '_classification_report.pkl')\n",
    "print(classification_report)\n",
    "\n",
    "# Convert classification report string to dataframe\n",
    "classification_report_lines = classification_report.split('\\n')\n",
    "# split on spaces within and drop blanks\n",
    "classification_report_data = [line.split() for line in classification_report_lines if line]\n",
    "# drop lists begining with 'precision', 'accuracy', 'macro', 'weighted'\n",
    "classification_report_data = [line for line in classification_report_data if line[0] not in ['precision', 'accuracy', 'macro', 'weighted']]\n",
    "# Stack list of rows into dataframe\n",
    "classification_report_data = pd.DataFrame(classification_report_data)\n",
    "# Set columns to \"Rating\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"\n",
    "classification_report_data.columns = ['Rating', 'Precision', 'Recall', 'F1-Score', 'Support']\n",
    "print(classification_report_data)\n",
    "\n",
    "# Export to Excel\n",
    "classification_report_data.to_excel('../../../../Output/Modelling/Logistic Regression/Tables/Most_Complex_Model_Classification_Report.xlsx', index=False)\n",
    "\n",
    "# Export to Latex\n",
    "#classification_report_data.to_latex('../../../../Output/Modelling/Logistic Regression/Tables/Most_Complex_Model_Classification_Report.tex', index=False)\n",
    "\n",
    "# Export to LaTeX\n",
    "# Format columns\n",
    "for col in classification_report_data.columns:\n",
    "    classification_report_data[col] = classification_report_data[col].apply(lambda x: f'{x:.2f}' if isinstance(x, float) else x)\n",
    "# Center all columns\n",
    "lt_string = classification_report_data.to_latex(index=False, column_format='x{0.75cm}|x{0.75cm}|x{0.75cm}|x{0.75cm}|x{0.75cm}', escape=False)\n",
    "latex_with_font_size = \"\\\\small\\n\" + lt_string + \"\\n\\\\normalsize\"\n",
    "with open('../../../../Output/Modelling/Logistic Regression/Tables/Most_Complex_Model_Classification_Report.tex', 'w') as f:\n",
    "    f.write(latex_with_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Complex Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1, 'class_weight': 'balanced', 'l1_ratio': 1.0, 'multi_class': 'ovr', 'penalty': 'elasticnet', 'solver': 'saga'}\n",
      "     C Class Weighting Strategy  L1 Ratio Multi-Class Strategy      Penalty  \\\n",
      "0  0.1                 Balanced       1.0          One vs Rest  Elastic Net   \n",
      "\n",
      "  Solver  \n",
      "0   SAGA  \n"
     ]
    }
   ],
   "source": [
    "# Load pickle '../../../../Output/Modelling/Logistic Regression/' + most_complex_model + '/' + most_complex_model + '_best_params.pkl'\n",
    "best_params = pd.read_pickle('../../../../Output/Modelling/Logistic Regression/' + most_complex_model + '/' + most_complex_model + '_best_params.pkl')\n",
    "print(best_params)\n",
    "\n",
    "# Convert to dataframe\n",
    "best_params = pd.DataFrame(best_params, index=[0])\n",
    "# Set columns to \"C\", \"Class Weighting Strategy\", \"L1 Ratio\", \"Multi-Class Strategy\", \"Penalty\", \"Solver\"\n",
    "best_params.columns = ['C', 'Class Weighting Strategy', 'L1 Ratio', 'Multi-Class Strategy', 'Penalty', 'Solver']\n",
    "# Replace 'Multi-Class Strategy' values\n",
    "best_params['Multi-Class Strategy'] = best_params['Multi-Class Strategy'].replace({'ovr': 'One vs Rest', 'multinomial': 'Multinomial'})\n",
    "# Replace 'Penalty' values\n",
    "best_params['Penalty'] = best_params['Penalty'].replace({'l1': 'L1', 'l2': 'L2', 'elasticnet': 'Elastic Net', 'none': 'None'})\n",
    "# Replace 'Solver' values\n",
    "best_params['Solver'] = best_params['Solver'].replace({'newton-cg': 'Newton Conjugate Gradient', 'lbfgs': 'Limited Memory Broyden–Fletcher–Goldfarb–Shanno', 'liblinear': 'Library for Large Linear Classification', 'sag': 'Stochastic Average Gradient', 'saga': 'SAGA'})\n",
    "# Replace Class Weighting Strategy values\n",
    "best_params['Class Weighting Strategy'] = best_params['Class Weighting Strategy'].replace({'balanced': 'Balanced', None: 'None'})\n",
    "print(best_params)\n",
    "\n",
    "# Export to Excel\n",
    "best_params.to_excel('../../../../Output/Modelling/Logistic Regression/Tables/Most_Complex_Model_Best_Params.xlsx', index=False)\n",
    "\n",
    "# Export to Latex\n",
    "#best_params.to_latex('../../../../Output/Modelling/Logistic Regression/Tables/Most_Complex_Model_Best_Params.tex', index=False)\n",
    "\n",
    "# Export to LaTeX\n",
    "for col in ['C', 'L1 Ratio']:\n",
    "    best_params[col] = best_params[col].apply(lambda x: '{:,.2f}'.format(x))\n",
    "# Center all columns\n",
    "lt_string = best_params.to_latex(index=False, column_format='x{0.75cm}|x{0.75cm}|x{0.75cm}|x{0.75cm}|x{0.75cm}|x{0.75cm}', escape=False)\n",
    "latex_with_font_size = \"\\\\small\\n\" + lt_string + \"\\n\\\\normalsize\"\n",
    "with open('../../../../Output/Modelling/Logistic Regression/Tables/Most_Complex_Model_Best_Params.tex', 'w') as f:\n",
    "    f.write(latex_with_font_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact predictions share == accuracy: True\n",
      "  Model/Baseline  Accuracy Weighted Average Precision Weighted Average Recall  \\\n",
      "0   Change Model      0.96                       0.91                    0.96   \n",
      "\n",
      "   F1 Score  Share 1 Rating Or Less From Actual  \n",
      "0      0.93                                 1.0  \n",
      "0.96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weighted Average Precision</th>\n",
       "      <th>Weighted Average Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Majority Baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Accuracy Weighted Average Precision Weighted Average Recall F1 Score  \\\n",
       "0     0.96                       0.91                    0.96     0.93   \n",
       "\n",
       "  Majority Baseline  \n",
       "0              0.96  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can start with output of get_model_comparison_row\n",
    "starter_row, maj_baseline = get_model_comparison_row('change_model', 'Change Model')\n",
    "print(starter_row)\n",
    "print(maj_baseline)\n",
    "\n",
    "# Drop Model/Baseline, Share Less Than 1 Rating From Actual\n",
    "changes_table = starter_row.drop(columns=['Model/Baseline', 'Share 1 Rating Or Less From Actual'])\n",
    "# Add column for maj_baseline\n",
    "changes_table['Majority Baseline'] = maj_baseline\n",
    "\n",
    "# Output to Excel\n",
    "changes_table.to_excel('../../../../Output/Modelling/Logistic Regression/Tables/changes_table.xlsx', index=False)\n",
    "\n",
    "# Output to Latex\n",
    "#changes_table.to_latex('../../../../Output/Modelling/Logistic Regression/Tables/changes_table.tex', index=False)\n",
    "\n",
    "# Export to LaTeX\n",
    "for col in ['Accuracy', 'F1 Score', 'Majority Baseline']:\n",
    "    changes_table[col] = changes_table[col].apply(lambda x: '{:,.2f}'.format(x))\n",
    "# Center all columns\n",
    "lt_string = changes_table.to_latex(index=False, column_format='x{0.75cm}|x{0.75cm}|x{0.75cm}|x{0.75cm}|x{0.75cm}', escape=False)\n",
    "latex_with_font_size = \"\\\\small\\n\" + lt_string + \"\\n\\\\normalsize\"\n",
    "with open('../../../../Output/Modelling/Logistic Regression/Tables/changes_table.tex', 'w') as f:\n",
    "    f.write(latex_with_font_size)\n",
    "\n",
    "changes_table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
