{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Data - NLP EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag for if you are running this on the sample dataset\n",
    "# Sample comprises 100 earnings calls (transcripts included)\n",
    "# Full data comprises 4532 earnings calls (transcripts included)\n",
    "sample = False\n",
    "# Modify this path as needed to run on your machine\n",
    "sample_path = r'~\\Box\\STAT 222 Capstone\\Intermediate Data\\all_data_sample.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ijyli\\\\Box\\\\STAT 222 Capstone\\\\Intermediate Data\\\\all_data.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(sample_path)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 6\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m~\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mBox\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSTAT 222 Capstone\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mIntermediate Data\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mall_data.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m      7\u001b[0m df\n",
      "File \u001b[1;32mc:\\Users\\ijyli\\anaconda3\\envs\\capstone\\Lib\\site-packages\\pandas\\io\\parquet.py:670\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    668\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ijyli\\anaconda3\\envs\\capstone\\Lib\\site-packages\\pandas\\io\\parquet.py:265\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    263\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    273\u001b[0m         path_or_handle,\n\u001b[0;32m    274\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    278\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ijyli\\anaconda3\\envs\\capstone\\Lib\\site-packages\\pandas\\io\\parquet.py:139\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[1;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[0;32m    129\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\ijyli\\anaconda3\\envs\\capstone\\Lib\\site-packages\\pandas\\io\\common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ijyli\\\\Box\\\\STAT 222 Capstone\\\\Intermediate Data\\\\all_data.parquet'"
     ]
    }
   ],
   "source": [
    "# Load in sample csv, or full parquet file\n",
    "# Use inputted sample path, or ~\\Box\\STAT 222 Capstone\\Intermediate Data\\all_data.parquet\n",
    "if sample:\n",
    "    df = pd.read_csv(sample_path)\n",
    "else:\n",
    "    df = pd.read_parquet(r'~\\Box\\STAT 222 Capstone\\Intermediate Data\\all_data.parquet') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Statistics\n",
    "num_records = len(df)\n",
    "avg_length = df['transcript'].str.len().mean()\n",
    "\n",
    "print(f\"Number of records: {num_records}\")\n",
    "print(f\"Average transcript length: {avg_length:.2f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "nltk.download('punkt')  # Download NLTK tokenizer data\n",
    "tokens = df['transcript'].apply(word_tokenize)\n",
    "\n",
    "# Remove stop words\n",
    "nltk.download('stopwords')  # Download NLTK stop words data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = tokens.apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words and word.isalpha()])\n",
    "\n",
    "# Number of sentences\n",
    "sentences = df['transcript'].apply(nltk.sent_tokenize)\n",
    "num_sentences = sentences.apply(len)\n",
    "print('average number of sentences:', num_sentences.mean())\n",
    "\n",
    "# Sentence length\n",
    "sentence_lengths = sentences.apply(lambda x: [len(word_tokenize(sentence)) for sentence in x])\n",
    "sentence_lengths = sentence_lengths.explode()\n",
    "print('average sentence length:', sentence_lengths.mean())\n",
    "\n",
    "# Word Frequency Analysis\n",
    "all_words = [word.lower() for token_list in tokens for word in token_list]\n",
    "fdist = FreqDist(all_words)\n",
    "top_words = fdist.most_common(10)\n",
    "print(\"Top 10 most common words:\")\n",
    "for word, freq in top_words:\n",
    "    print(f\"{word}: {freq}\")\n",
    "\n",
    "# Plot Word Frequency Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "fdist.plot(30, title='Top 30 Most Common Words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Length Distribution\n",
    "transcript_lengths = df['transcript'].str.len()\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(transcript_lengths, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Transcript Length Distribution')\n",
    "plt.xlabel('Transcript Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Number of Words in earning calls \n",
    "def count_words(tokens):\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    return len(tokens)\n",
    "\n",
    "word_tokens = df['transcript'].apply(word_tokenize)\n",
    "num_words = word_tokens.apply(count_words)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(num_words, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Words Distribution')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print('Average transcript length in words:', num_words.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Generate word clouds for each credit rating\n",
    "ratings = [\"AA\",\"BBB\",\"CCC\"]\n",
    "for rating in ratings:\n",
    "\n",
    "    rating_df = df[df['Rating'] == rating]\n",
    "    tokens = rating_df['transcript'].apply(word_tokenize)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = tokens.apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words and word.isalpha()])\n",
    "    \n",
    "    all_words = [word.lower() for token_list in tokens for word in token_list]\n",
    "    fdist = FreqDist(all_words)\n",
    "\n",
    "    # Generate Word Cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(fdist)\n",
    "    \n",
    "    # Plot Word Cloud\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Word Cloud for Credit Rating {rating}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from word cloud for credit rating AA, postive words like \"growth\" are larger when compared to credit rating CCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams and trigrams\n",
    "# Generate for each credit rating\n",
    "ratings = [\"AA\",\"BBB\",\"CCC\"]\n",
    "for rating in ratings:\n",
    "\n",
    "    rating_df = df[df['Rating'] == rating]\n",
    "\n",
    "    # tokens = rating_df['transcript'].apply(word_tokenize)\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = tokens.apply(lambda tokens: [word for word in tokens if word.lower() not in stop_words and word.isalpha()])\n",
    "    \n",
    "    # all_words = [word.lower() for token_list in tokens for word in token_list]\n",
    "\n",
    "    cv = CountVectorizer(ngram_range=(2,2))\n",
    "    bigrams = cv.fit_transform(rating_df['transcript'])\n",
    "    count_values = bigrams.toarray().sum(axis=0)\n",
    "    ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv.vocabulary_.items()], reverse = True))\n",
    "    ngram_freq.columns = [\"frequency\", \"ngram\"]\n",
    "    # toss out items if they only contain stop words\n",
    "    ngram_freq = ngram_freq[ngram_freq['ngram'].apply(lambda x: all(word in stop_words for word in x.split())) == False]\n",
    "    sns.barplot(x=ngram_freq['frequency'][:10], y=ngram_freq['ngram'][:10])\n",
    "    plt.title('Top 10 Most Frequently Occuring Bigrams for Rating ' + rating)\n",
    "    plt.show()\n",
    "\n",
    "    # cv1 = CountVectorizer(ngram_range=(3,3))\n",
    "    # trigrams = cv1.fit_transform(rating_df['transcript'])\n",
    "    # count_values = trigrams.toarray().sum(axis=0)\n",
    "    # ngram_freq = pd.DataFrame(sorted([(count_values[i], k) for k, i in cv1.vocabulary_.items()], reverse = True))\n",
    "    # ngram_freq.columns = [\"frequency\", \"ngram\"]\n",
    "    # # toss out items if they only contain stop words\n",
    "    # ngram_freq = ngram_freq[ngram_freq['ngram'].apply(lambda x: all(word in stop_words for word in x.split())) == False]\n",
    "    # sns.barplot(x=ngram_freq['frequency'][:10], y=ngram_freq['ngram'][:10])\n",
    "    # plt.title('Top 10 Most Frequently Occuring Trigrams for Rating ' + rating)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank yous become far less common as credit rating deteriorates!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
